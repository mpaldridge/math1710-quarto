{
  "hash": "7f63e64456a4ed5e598fb4b4a3d286ef",
  "result": {
    "markdown": "# Exponential distribution and multiple continuous random variables  {#L17-exp-multiple}\n\nThis lecture is really two mini-lectures stuck together. In the first mini-lecture, we look at another important continuous distribution, the exponential distribution. In the second mini-lecture, we look at the theory of multiple continuous random variables, and find it's very similar to what we already know about multiple discrete random variables.\n\n## Exponential distribution  {#exponential}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/t11Rsll3jVI\"></iframe>\n:::\n::::\n-->\n\n\nAn important continuous distribution is the exponential distribution. The exponential distribution is often used to represent lengths of time: for example, the time between radioactive particles decaying, the time between eruptions of a volcano, or the time between buses arriving at a bus stop.\n\n::: {.definition}\nA continuous random variable $X$ is said to have the **exponential distribution with rate $\\lambda > 0$** if it has the PDF\n\\[ f(x) = \\lambda \\mathrm{e}^{-\\lambda x} \\qquad \\text{for $x \\geq 0$}, \\]\nand 0 otherwise. We write $X \\sim \\text{Exp}(\\lambda)$.\n:::\n\n\n::: {.cell hash='L16-exponential-multi_cache/html/exp-pic_99cc2db5b5d259ea7b2e2a4d394d1a22'}\n::: {.cell-output-display}\n![](L16-exponential-multi_files/figure-html/exp-pic-1.png){width=672}\n:::\n:::\n\n\n::: {.example}\n*The length of time in years that a lightbulb works before needing to be replaced is modelled as an exponential distribution with rate $\\lambda = 2$. What is the probability the lightbulb needs replacing within a year?*\n\nIf $X \\sim \\text{Exp}(2)$ is the lifetime of the lightbulb, we seek $\\mathbb P(X \\leq 1)$. This is\n\\[ \\int_{-\\infty}^1 f(x)\\, \\mathrm{d}x = \\int_0^1 2 \\mathrm e^{-2x} \\, \\mathrm dx = \\big[ -\\mathrm e^{-2x} \\big]_0^1 = -\\mathrm e^{-2} -(-1) = 1 - \\mathrm e^{-2} = 0.864.  \\]\n:::\n\n::: {.theorem #exp-prop}\nSuppose $X \\sim \\text{Exp}(\\lambda)$. Then:\n\n1. $f$ is indeed a PDF, in that $\\displaystyle\\int_0^\\infty f(x)\\,\\mathrm{d}x = 1$;\n2. the CDF of $X$ is $F(x) = 1 - \\mathrm{e}^{-\\lambda x}$;\n3. the expectation of $X$ is $\\mathbb EX = \\displaystyle\\frac{1}{\\lambda}$;\n4. the variance of $X$ is $\\Var(X) = \\displaystyle\\frac{1}{\\lambda^2}$.\n:::\n\n::: {.example}\nReturning to the lightbulb example, where $X \\sim \\text{Exp}(2)$, we see that the average lifetime of a lightbulb is $\\mathbb EX = \\frac12$ a year with variance $\\Var(X) = \\frac14$.\n\nIf we wanted to calculate $\\mathbb P(1 \\leq X \\leq 3)$, we could do this by integrating the PDF between 1 and 3. Alternatively, we could use the CDF:\n\\[ \\mathbb P(1 \\leq X \\leq 3) = F(3) - F(1) = (1 - \\mathrm{e}^{-2\\times 3}) - (1 - \\mathrm{e}^{-2\\times 1}) = \\mathrm{e}^{-2} - \\mathrm{e}^{-6} = 0.132 . \\]\n:::\n\n::: {.proof}\n*of Theorem \\@ref(thm:exp-prop).*\nFor part 1,\n\\[ \\int_0^\\infty \\lambda \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x\n     = \\big[-\\mathrm{e}^{-\\lambda x} \\big]_0^\\infty = -0 -(-1) = 1 . \\]\n\nSimilarly for part 2,\n\\[ F(x) = \\int_0^x \\lambda \\mathrm{e}^{-\\lambda y}\\,\\mathrm{d}y\n     = \\big[-\\mathrm{e}^{-\\lambda y} \\big]_0^x = -\\mathrm{e}^{-\\lambda x} -(-1) = 1 - \\mathrm{e}^{-\\lambda x}. \\]\n     \nFor part 3, we use integration by parts with $u = x$ and $v' = \\lambda \\mathrm{e}^{-\\lambda x}$, so $u' = 1$ and $v = -\\mathrm{e}^{-\\lambda x}$. We get\n\\begin{align*}\n\\mathbb EX &= \\int_0^\\infty x  \\lambda \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x \\\\\n  &= \\big[-x \\mathrm{e}^{-\\lambda x}\\big]_0^\\infty + \\int_0^\\infty \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x \\\\\n  &= -0 - (-0) + \\left[ -\\frac{1}{\\lambda} \\mathrm{e}^{-\\lambda x} \\right]_0^\\infty \\\\\n  &= -0 - \\left(- \\frac{1}{\\lambda}\\right) \\\\\n  &= \\frac{1}{\\lambda}\n\\end{align*}\n\nFor part 4, we use integration by parts with $u = x^2$ and $v' = \\lambda \\mathrm{e}^{-\\lambda x}$, so $u' = 2x$ and $v = -\\mathrm{e}^{-\\lambda x}$ again. We get\n\\begin{align*}\n\\mathbb EX^2 &= \\int_0^\\infty x^2  \\lambda \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x \\\\\n  &= \\big[-x^2 \\mathrm{e}^{-\\lambda x}\\big]_0^\\infty + \\int_0^\\infty 2x \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x \\\\\n  &= -0 - (-0) + \\frac{2}{\\lambda} \\int_0^\\infty x  \\lambda x \\mathrm{e}^{-\\lambda x}\\,\\mathrm{d}x \\\\\n  &= \\frac{2}{\\lambda} \\mathbb EX \\\\\n  &= \\frac{2}{\\lambda^2} ,\n\\end{align*}\nwhere we used a cunning trick on the integral on the right -- spotting that we could turn it into the expectation, which is $1/\\lambda$, by part 3 -- to save us the effort of calculating it again.\nHence\n\\[ \\Var(X) = \\mathbb EX^2 - \\left(\\frac{1}{\\lambda}\\right)^2 =  \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2} .  \\]\n:::\n\n\n\n## Multiple continuous random variables  {#continuous-multiple}\n\nThe theory we set up for two or more discrete random variables also works for two or more continuous random variables.\n\nNow, the intensity of probability for $(X,Y)$ being around $(x,y)$ is given by the **joint probability density function** $f_{X,Y}$. In particular for $a \\leq b$ and $c \\leq d$, we have\n\\[ \\mathbb P(a \\leq X \\leq b \\text{ and } c \\leq Y \\leq d ) = \\int_{x = a}^b \\int_{y = c}^d f_{X,Y}(x,y)\\, \\mathrm dx \\,\\mathrm dy .\\]\n\n|   Discrete random variables   |   Continuous random variables   |\n|-----|-----|\n| We can get the **marginal PMF** $p_X$ of $X$ by summing over $y$, so \\[ p_X(x) = \\sum_y p_{X,Y}(x,y) . \\] | We can get the **marginal PDF** $f_X$ of $X$ by integrating over $y$, so \\[ f_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x,y) \\, \\mathrm dy. \\] |\n| Two discrete random variables $X$ and $Y$ are **independent** if their PMFs satisfy \\[p_{X,Y}(x,y) = p_X(x)\\,p_Y(y) \\qquad \\text{for all $x, y$}.\\] | Two continuous random variables $X$ and $Y$ are **independent** if they have PDFs which satisfy \\[f_{X,Y}(x,y) = f_X(x)\\,f_Y(y) \\qquad \\text{for all $x, y$}.\\] |\n| The **conditional PMF** of $Y$ given $X$ is defined by \\[ p_{Y \\mid X}(y \\mid x) = \\frac{p_{X,Y}(x,y)}{p_X(x)} . \\] | The **conditional PDF** of $Y$ given $X$ is defined by \\[ f_{Y \\mid X}(y \\mid x) = \\frac{f_{X,Y}(x,y)}{f_X(x)} . \\] |\n| **Bayes' theorem** states that \\[ p_{X \\mid Y}(x \\mid y) = \\frac{p_X(x)\\,p_{Y\\mid X}(y\\mid x)}{p_Y(y)} . \\] | **Bayes' theorem** states that \\[ f_{X \\mid Y}(x \\mid y) = \\frac{f_X(x)\\,f_{Y\\mid X}(y\\mid x)}{f_Y(y)} . \\] |\n| The expectation of a function of $X$ and $Y$ is given by the sum \\[ \\mathbb Eg(X,Y) = \\sum_{x,y} g(x,y)\\, p_{X,Y}(x,y) . \\] | The expectation of a function of $X$ and $Y$ is given by the integral \\[ \\mathbb Eg(X,Y) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y)\\, f_{X,Y}(x,y) \\, \\mathrm dx \\, \\mathrm dy . \\] |\n| The **covariance** of $X$ and $Y$ is given by \\[ \\Cov(X,Y) = \\mathbb E(X - \\mu_X)(Y - \\mu_Y) , \\] and has a computational formula \\[ \\Cov(X,Y) = \\mathbb EXY - \\mu_X \\mu_Y . \\] | The **covariance** of $X$ and $Y$ is given by \\[ \\Cov(X,Y) = \\mathbb E(X - \\mu_X)(Y - \\mu_Y) , \\] and has a computational formula \\[ \\Cov(X,Y) = \\mathbb EXY - \\mu_X \\mu_Y . \\]\n\n::: {.example}\nConsider the pair of continuous random variable $(X,Y)$ with joint PDF\n\\[ f_{X,Y}(x,y) = \\tfrac12(1 + x + y) \\qquad \\text{for $0 \\leq x,y\\leq 1$} \\]\nand $f_{X,Y}(x,y) = 0$ otherwise.\n\nWe get the marginal distribution for $X$ by integrating over $y$, so\n\\[ f_X(x) = \\int_0^1 \\tfrac12(1 + x + y) \\, \\mathrm dy = \\tfrac12 \\left[(1 + x)y + \\tfrac12y^2\\right]_0^1 = \\tfrac34 + \\tfrac12x . \\]\n\nWe can find the the conditional PDF for $Y$ given $X = \\tfrac14$. It is\n\\[ f_{Y\\mid X}\\big(y \\mid \\tfrac14\\big) = \\frac{f_{X,Y}\\big(\\tfrac14,y\\big)}{f_X\\big(\\tfrac14\\big)}\n    = \\frac{\\tfrac12\\big(1 + \\tfrac14 + y\\big)}{\\tfrac34 + \\tfrac12\\times\\tfrac14 } = \\tfrac{5}{7} + \\tfrac47 y . \\]\n    \nWe can calculate the covariance. First, the expectations are\n\\[ \\mathbb EX = \\int_{-\\infty}^\\infty x\\, f_X(x) \\,\\mathrm dx = \\int_0^1 x\\big(\\tfrac34 + \\tfrac12x\\big)\\, \\mathrm dx = \\left[\\tfrac38 x^2 + \\tfrac16 x^3 \\right] = \\tfrac{13}{24} \\]\nand $\\mathbb EY = \\frac{13}{24}$ also, by symmetry. Second, we have\n\\begin{align*}\n\\mathbb EXY\n&= \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty xy\\, f_{X,Y}(x,y) \\, \\mathrm dx\\, \\mathrm dy \\\\\n&= \\int_0^1 \\int_0^1 xy \\, \\tfrac12(1 + x + y)\\, \\mathrm dx\\, \\mathrm dy \\\\\n&= \\int_0^1 \\left[ \\tfrac14 x^2y + \\tfrac16 x^3y + \\tfrac14 x^2y^2  \\right]_{x=0}^1 \\, \\mathrm dy\\\\\n&= \\int_0^1 \\big( \\tfrac14 y + \\tfrac16 y + \\tfrac14 y^2 \\big) \\, \\mathrm dy\\\\\n&= \\left[ \\tfrac18 y^2 + \\tfrac{1}{12}y^2 + \\tfrac{1}{12}y^3  \\right]_0^1 \\\\\n&= \\tfrac{7}{24} .\n\\end{align*}\nSo therefore,\n\\[ \\Cov(X,Y) = \\mathbb EXY - \\mu_X \\mu_Y = \\tfrac{7}{24} - \\tfrac{13}{24} \\times \\tfrac{13}{24} = -\\tfrac{1}{576} . \\]\n::::\n\n## Summary  {#summary-L17 .unnumbered}\n\n::: {.mysummary}\n* The exponential distribution has PDF $f(x) = \\lambda \\mathrm e^{-\\lambda x}$, expectation $1/\\lambda$, and variance $1/\\lambda^2$.\n* Most properties of multiple discrete random variables carry of to multiple continuous random variables. To get a marginal PDF from a joint PDF, we integrate (rather than sum) over the other variable.\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}