{
  "hash": "81bb9ecf24c54e4de8f6316c91862ec8",
  "result": {
    "markdown": "# More Bayesian models {#L20-bayes-models}\n\n## Beta distribution  {#beta}\n\nIn our fake-coin example in the last lecture, we had a prior PMF for the parameter $\\theta = p$ that could only take one of three possible values. But when doing Bayesian statistics with a parameter that represents a probability, it makes more sense to have a prior PDF that covers the whole interval $[0,1]$. After all, any parameter value that is given a probability of 0 in the prior always has a probability 0 in the posterior as well, no matter how strong the evidence in its favour; it's considered good practice to only put 0 prior probability on parameter values that are *literally impossible*, such as probabilities below 0 or above 1. (This is sometimes called [\"Cromwell's rule\"](https://en.wikipedia.org/wiki/Cromwell%27s_rule).)\n\nOne useful family of distributions to use as a prior distribution for a probability parameter is the Beta distribution, whose range is the whole interval $[0,1]$.\n\n::: {.definition}\nA continuous random variable $X$ is said to have the **Beta distribution** with parameters $\\alpha$ and $\\beta$ if it has the PDF\n\\[ f(x) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta - 1} \\qquad \\text{for $0 \\leq x \\leq 1$}  \\]\nand 0 otherwise. Here, the constant\n\\[ B(\\alpha, \\beta) = \\int_0^1 x^{\\alpha-1} (1-x)^{\\beta - 1} \\, \\mathrm dx , \\]\nknown as the \"Beta function\", ensures that the PDF integrates to 1. We write $X \\sim \\text{Beta}(\\alpha, \\beta)$.\n:::\n\n::: {.theorem}\nLet $X \\sim \\text{Beta}(\\alpha,\\beta)$. Then\n\n1. $\\mathbb EX = \\displaystyle\\frac{\\alpha}{\\alpha + \\beta}$\n2. $\\Var(X) = \\displaystyle\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} = \\displaystyle\\frac{\\mu(1-\\mu)}{\\alpha+\\beta + 1}$, where $\\mu = \\mathbb EX$.\n:::\n\n(Proving this requires some awkward messing around with Gamma functions, which we won't bother with here.)\n\nSo the idea is that the expectation of $X$ is decided on by the *relative* values of $\\alpha$ and $\\beta$, while the variance is decided by the *total* value of $\\alpha$ and $\\beta$. The following two pictures illustrate this:\n\n\n::: {.cell hash='L20-bayes-models_cache/html/beta-pic-1_938585bac3b11d75db88598980acce3b'}\n::: {.cell-output-display}\n![](L20-bayes-models_files/figure-html/beta-pic-1-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='L20-bayes-models_cache/html/beta-pic-2_0352e43912f211d611450c75a1a7395e'}\n::: {.cell-output-display}\n![](L20-bayes-models_files/figure-html/beta-pic-2-1.png){width=672}\n:::\n:::\n\n\nNote also that $\\text{Beta}(1,1)$ is the continuous uniform distribution from Example \\@ref(exm:unifex).\n\n::: {.example}\n*A statistician is studying the probability $\\theta$ that ordinary coins land Heads. She would like to use a prior distribution for $\\theta$ with prior expectation $0.5$ and prior standard deviation $0.01$. What Beta distribution would be appropriate to use?*\n\nTo get $\\mathbb E\\theta = 0.5$, we need $\\alpha = \\beta$. Then the variance, which needs to be $0.01^2 = 0.0001$, is\n\\[ \\Var(\\theta) = \\frac{\\mu(1-\\mu)}{\\alpha+\\beta+1} = \\frac{0.25}{\\alpha + \\beta + 1} . \\]\nThis requires $\\alpha = \\beta = 1250$. (Well, actually $1249.5$.)\n:::\n\n## Beta--Bernoulli model  {#beta-bern}\n\nConsider a Bernoulli likelihood, where $X_1, X_2, \\dots, X_n$ are IID $\\text{Bern}(\\theta)$, so have joint PMF\n\\[ p(\\mathbf x \\mid \\theta) = \\prod_{i=1}^n \\theta^{x_i} (1-\\theta)^{1 - x_i} = \\theta^{\\sum_i x_i} (1 - \\theta)^{n-\\sum_i x_i} = \\theta^y (1 - \\theta)^{n-y}, \\]\nwhere we have written $y = \\sum_i x_i$ for the total number of successes.\nConsider further using a $\\text{Beta}(\\alpha, \\beta)$ prior for $\\theta$, so that\n\\[ \\pi(\\theta) = \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta - 1} \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta - 1} \\]\n(Because we're going to use the \"posterior has to add up to 1\" trick at the end, we're free to drop constants whenever we want.) This is known as the **Beta--Bernoulli model**.\n\nSuppose we collect data $\\mathbf x = (x_1, x_2, \\dots, x_i)$, with $y = \\sum_i x_i$ successes. What now is the posterior distribution for $\\theta$ given this data?\n\nUsing Bayes' theorem, we have\n\\begin{align*}\n\\pi(\\mathbf x \\mid \\theta)\n  &\\propto \\pi(\\theta) p(\\mathbf x \\mid \\theta) \\\\\n  &= \\theta^{\\alpha-1} (1-\\theta)^{\\beta - 1} \\times \\theta^y (1 - \\theta)^{n-y} \\\\\n  &= \\theta^{\\alpha + y - 1} (1 - \\theta)^{\\beta + n - y - 1} .\n\\end{align*}\nWe can recognise immediately that this is proportional to the PDF for a $\\text{Beta}(\\alpha + y, \\beta + n - y)$ distribution, so in particular, the constant of proportionality must be $1/B(\\alpha + y, \\beta + n - y)$.\n\nSo we see that, like the prior, the posterior is also a Beta distribution, where the first parameter has gone from $\\alpha$ to $\\alpha + y$ and the second parameter has gone from $\\beta$ to $\\beta + (n-y)$. In other words, $\\alpha$ has increased by the number of successes, and $\\beta$ has increased by the number of failures.\nThe expectation has gone from the prior expectation\n\\[ \\frac{\\alpha}{\\alpha + \\beta} \\]\nto the posterior expectation\n\\[ \\frac{\\alpha + y}{\\alpha + \\beta + n} .\\]\nThis can be thought of as a sort of average between the prior expectation $\\alpha/(\\alpha + \\beta)$ and the mean of the data $y/n$.\n\n## Modern Bayesian statistics  {#modern-bayes}\n\nIn this section, we've given just a brief taster of Bayesian statistics. Bayesian statistics is a deep and complicated subject, and you may have the opportunity to find out a lot more about it later in your university career.\n\nWe have seen that in Bayesian statistics, one brings in a subjective \"prior\" based on previous beliefs and evidence, then updates this prior based on the data. This contrasts with the more traditional **frequentist statistics**. In frequentist one uses only the data -- no prior beliefs! -- and judges to what extent the data is consistent or inconsistent with a hypothesis, without weighing in on how likely such a hypothesis is. (Frequentist statistics is the main subject studied in MATH1712 Probability and Statistics II.)\n\nIn the two main examples of Bayesian statistics we have looked at -- the Bernoulli likelihood and the normal likelihood -- we ended up with a posterior in the same parametric family as prior, just with different parameters. Such a prior is called a \"conjugate prior\". Of course, these are very convenient and easy to work with. However, with more complicated likelihoods and more complicated priors -- especially those not with a single parameter but with many parameters -- calculating the posterior distribution can be very difficult. In particular, working out the constant of proportionality (even just approximately) and/or sampling from the posterior distribution are very hard problems.\n\nFor this reason, Bayesian statistics was for a long time a minor area of statistics. However, increases in computer power in the 1980s made some of these problems more tractable, and Bayesian statistics has increased in importance and popularity since then.\n\nFor a while, there was an occasionally fierce debate between \"Bayesians\" and \"frequentists\". Frequentists thought that bringing subjective personal beliefs into things was unmathematical, while Bayesians thought that ignoring how plausible a hypothesis is before testing it is unscientific. The debate has now largely dissipated, and it is largely accepted that modern statisticians need to know about both frequentist and Bayesian methods.\n\nThere are still plenty of open problems in Bayesian statistics, and lots of these involve the computational side: finding algorithms that can efficiently calculate the normalising constants in posterior distributions or sample from those posterior distributions, especially when the parameter(s) have very high dimension.\n\n\n## Summary  {#summary-10 .unnumbered}\n\n::: {.mysummary}\n* In Bayesian statistics, we start with a prior distribution for a parameter $\\theta$, and update to a posterior distribution given the data $\\mathbf x$, through $\\pi(\\theta \\mid \\mathbf x) \\propto \\pi(\\theta)p(\\mathbf x \\mid \\theta)$, or $\\text{posterior} \\propto \\text{prior} \\times \\text{likelihood}$.\n* The Beta distribution is a useful family of distributions to use as priors for probability parameters.\n* A Beta prior for a Bernoulli likelihood leads to a Beta posterior with different parameters.\n* A normal prior for the expectation of a normal likelihood wioth known variance leads to a normal posterior with different parameters.\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}