{"title":"Expectation and covariance","markdown":{"headingText":"Expectation and covariance ","headingAttr":{"id":"L14-covariance","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Expectation of sums and products {#sum-product}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/BaEbVJZkGGY\"></iframe>\n:::\n::::\n-->\n\nWhen we have multiple random variables, we might be interested in functions of those multiple random variables -- for example their sum or their product. It's often possible to find out about the whole distribution of a sum, product, or function of the variables -- see MATH2715 Statistical Methods for more on this -- but will just look at their expectations and, later, variances.\n\n::: {.theorem #linearity2}\nLet $X$ and $Y$ be two random variables with joint probability mass function $p_{X,Y}$. Then\n\n1. $\\mathbb Eg(X,Y) = \\displaystyle\\sum_{x,y} g(x,y) p_{X,Y}(x,y)$.\n2. **(Linearity of expectation, 2)** $\\mathbb E(X + Y) = \\mathbb EX + \\mathbb EY$, regardless of whether $X$ and $Y$ are independent or not.\n3. If $X$ and $Y$ are independent, then $\\mathbb EXY = \\mathbb EX \\times \\mathbb EY$.\n:::\n\nIf we put the second point here together with the other result of linearity of expectation (Theorem \\@ref(thm:linearity1)) then we get the general rule\n\\[ \\mathbb E(aX + bY + c) = a\\,\\mathbb EX + b \\,\\mathbb EY + c , \\]\nand this holds whether or not $X$ and $Y$ are independent.\n\n::: {.proof}\nPart 1 is just the law of the unconscious statistician for the random variable $(X,Y)$, and the same proof holds.\n\nFor part 2, we have\n\\begin{align*}\n\\mathbb E(X + Y) &= \\sum_{x,y} (x + y)p_{X,Y}(x,y) \\\\\n  &= \\sum_{x,y} x\\,p_{X,Y}(x,y) + \\sum_{x,y} y\\,p_{X,Y}(x,y) \\\\\n  &= \\sum_x x \\sum_y p_{X,Y}(x,y) + \\sum_y y \\sum_x p_{X,Y}(x,y)\n\\end{align*}\nBut summing a joint PMF over one of the variables gives the marginal PMF; so $\\sum_y p_{X,Y}(x,y) = p_X(x)$ and $\\sum_x p_{X,Y}(x,y) = p_Y(y)$. So this gives\n\\begin{align*}\n\\mathbb E(X + Y) &= \\sum_x x\\, p_X(x) + \\sum_y y\\,p_Y(y) \\\\\n&= \\mathbb EX + \\mathbb EY .\n\\end{align*}\n\nFor part 3, if $X$ and $Y$ are independent, then $p_{X,Y}(x,y) = p_X(x) \\, p_Y(y)$. Therefore,\n\\begin{align*}\n\\mathbb EXY &= \\sum_{x,y} xy p_{X,Y}(x,y) \\\\\n  &= \\sum_x \\sum_y xy p_X(x) p_Y(y) \\\\\n  &= \\sum_x x p_X(x) \\sum_y y p_Y(y) \\\\\n  &= \\mathbb EX \\times \\mathbb EY,\n\\end{align*}\nas required.\n:::\n\n::: {.example}\n*A student is solving five questions on a problem sheet. The time taken for each question to the nearest minute is an identically distributed random variable with expectation $\\mathbb EX_i = \\mu$. What is the total expected time to complete the problem sheet?*\n\nBy linearity of expectation, this is\n\\[ \\mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = \\mathbb EX_1 + \\mathbb EX_2 + \\mathbb EX_3 + \\mathbb EX_4 + \\mathbb EX_5 = 5\\mu . \\]\n\n*What if the lengths of time are not independent -- for example, if the student is slower at answering all the questions when she is tired?*\n\nIt's still the case that $\\mathbb E(X_1 + X_2 + X_3 + X_4 + X_5) = 5\\mu$, because this result is true whether or not the random variables are independent.\n:::\n\n\n\n## Covariance {#covariance}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/eXsUrJzcRCE\"></iframe>\n:::\n::::\n-->\n\nIf we are interested at how two random variables vary together, we need to look at the covariance.\n\n\n\\newcommand{\\Cov}{\\operatorname{Cov}}\n\\newcommand{\\Corr}{\\operatorname{Corr}}\n\n\n::: {.definition}\nLet $X$ and $Y$ be two random variables with expectations $\\mathbb EX =\\mu_X$ and $\\mathbb EY = \\mu_Y$ respectively. Then their **covariance** is\n\\[ \\Cov(X,Y) = \\mathbb E(X - \\mu_X)(Y - \\mu_Y) . \\]\n:::\n\nIn the least surprising result of this whole module, we also have a computational formula to go along with this definitional formula.\n\n::: {.theorem}\nLet $X$ and $Y$ be two random variables with expectations $\\mu_X$ and $\\mu_Y$ respectively. Then their covariance can also be calculated as\n\\[ \\Cov(X,Y) = \\mathbb EXY - \\mu_X\\, \\mu_Y . \\]\n:::\n\n::: {.proof}\nExactly as we've done many times before, we have\n\\begin{align*}\n\\Cov(X,Y) &= \\mathbb E(X - \\mu_X)(Y - \\mu_Y) \\\\\n&= \\mathbb E(XY - X\\,\\mu_Y - \\mu_X\\, Y + \\mu_X\\,\\mu_Y) \\\\\n&= \\mathbb EXY  - \\mu_Y \\,\\mathbb EX - \\mu_X \\,\\mathbb EY + \\mu_X \\, \\mu_Y \\\\\n&= \\mathbb EXY - \\mu_X \\, \\mu_Y - \\mu_X \\, \\mu_Y + \\mu_X \\, \\mu_Y \\\\\n&= \\mathbb EXY - \\mu_X \\, \\mu_Y ,\n\\end{align*}\nand we're done.\n:::\n\n::: {.example}\nWe continue with our coin-tossing example from the last lecture, where $X$ is the number of Heads in the first two coin tosses and $Y$ the number of Heads in the first three coin tosses.\n\nWe know that $X \\sim \\text{Bin}(2, \\frac12)$, so $\\mu_X = 1$, and  $Y \\sim \\text{Bin}(3, \\frac12)$, so $\\mu_Y = 1.5$. To find the covariance using the computational formula, we also need $\\mathbb EXY$, which is\n\\begin{align*}\n\\mathbb EXY &= \\sum_{x,y} xy\\, p_{X,Y}(x,y) \\\\\n  &= 0\\times 0\\times p_{X,Y}(0,0) + 0 \\times 1 \\times p_{X,Y}(0,1) + \\cdots + 2\\times 3 \\times p_{X,Y}(2,3) \\\\\n  &= 0 \\times \\tfrac18 + 0 \\times \\tfrac18 + \\cdots + 6 \\times \\tfrac18 \\\\\n  &= 2.\n\\end{align*}\nHence the covariance is\n\\[ \\Cov (X,Y) = \\mathbb EXY - \\mu_X\\mu_Y = 2 - 1 \\times 1.5 = 0.5 .\\]\n:::\n\n\nA very important fact is the following.\n\n::: {.theorem}\nIf $X$ and $Y$ are independent, then $\\Cov(X,Y) = 0$.\n:::\n\nBe careful not to get this the wrong way around: if $\\Cov(X,Y) = 0$ it doesn't necessarily mean that $X$ and $Y$ are independent.\n\nTo use the [\"contrapositive\"](https://www.varsitytutors.com/hotmath/hotmath_help/topics/converse-inverse-contrapositive) (which is allowed!), in our example, we have $\\Cov(X,Y) \\neq 0$, which means that $X$ and $Y$ are not independent -- confirming what we already knew.\n\n::: {.proof}\nRecall from Theorem \\@ref(thm:linearity2) that if $X$ and $Y$ are independent, we have $\\mathbb EXY = \\mathbb EX \\times \\mathbb EY = \\mu_X \\, \\mu_Y$. Then from the computational formula, we have\n\\[ \\Cov(X,Y) = \\mathbb EXY - \\mu_X\\,\\mu_Y = \\mu_X\\,\\mu_Y - \\mu_X\\,\\mu_Y = 0, \\]\nand we are done.\n:::\n\nHere are some more important properties of the covariance.\n\n::: {.theorem}\nLet $X$, $Y$ and $Z$ be random variables. Then\n\n1. $\\Cov(X,Y) = \\Cov(Y,X)$;\n2. $\\Cov(X,X) = \\Var(X)$;\n3. $\\Cov(aX, Y) = a\\,\\Cov(X,Y)$;\n4. $\\Cov(X + b, Y) = \\Cov(X,Y)$;\n5. $\\Cov(X + Y, Z) = \\Cov(X, Z) + \\Cov(Y,Z)$.\n:::\n\n::: {.proof}\nPart 1 and 2 are immediate from the definition.\n\nParts 3, 4 and 5 are quite similar. We'll do part 5 here, and you can do parts 3 and 4 on [Problem Sheet 4](#P4).\n\nFor part 5, note that $\\mathbb E(X + Y) = \\mu_X + \\mu_Y$ by linearity of expectation. Hence\n\\begin{align*}\n\\Cov(X + Y, Z)\n  &= \\mathbb E \\big((X + Y) - (\\mu_X + \\mu_Y)\\big)(Z - \\mu_Z) \\\\\n  &= \\mathbb E \\big((X - \\mu_X) + (Y - \\mu_Y)\\big)(Z - \\mu_Z) \\\\\n  &= \\mathbb E \\big((X - \\mu_X)(Z - \\mu_Z) + (Y - \\mu_Y) (Z - \\mu_Z) \\big) \\\\\n  &= \\mathbb E (X - \\mu_X)(Z - \\mu_Z) + \\mathbb E  (Y - \\mu_Y) (Z - \\mu_Z) \\\\\n  &= \\Cov(X,Z) + \\Cov(Y,Z) ,\n\\end{align*}\nas required.\n:::\n\nWe could calculate the covariance in our coin-tossing example a different way, by noting that $Y = X + Z$, where $Z \\sim \\text{Bern}(\\frac12)$ represents the third coin toss and is independent of $X$. Then we have\n\\[\n\\Cov(X,Y) = \\Cov(X, X + Z) = \\Cov(X, X) + \\Cov(X, Z) =\n= \\Var(X) + 0 = \\Var(X) ,\\]\nwhere  we used $\\Cov(X, Z) = 0$ since $X$ and $Z$ are independent.\nWe already know that $\\Var(X) = 2 \\times \\tfrac12 \\times (1 - \\tfrac12) = \\tfrac12$ because $X \\sim \\text{Bin}(2, \\frac12)$.. So $\\Cov(X,Y) = \\frac12$,\nmatching our previous calculation. \n\nNow that we know some facts about the covariance, we can calculate the variance of a sum.\n\n::: {.theorem}\nLet $X$ and $Y$ be two random variables. Then\n\\[ \\Var(X + Y) = \\Var(X) + 2\\Cov(X,Y) + \\Var(Y) . \\]\n\nIf $X$ and $Y$ are independent, then\n\\[ \\Var(X + Y) = \\Var(X) + \\Var(Y) . \\]\n:::\n\nIt's easy to forget the conditions for the following two facts:\n\n* $\\mathbb E(X + Y) = \\mathbb EX + \\mathbb EY$ regardless of whether $X$ and $Y$ are independent or not.\n* $\\Var(X+Y) = \\Var(X) + \\Var(Y)$ if $X$ and $Y$ are independent.\n\n::: {.proof}\nFor the main part of the proof, we start with the definition of variance. By linearity of expectation, we have $\\mathbb E(X + Y) = \\mu_X + \\mu_Y$. So\n\\begin{align*}\n\\Var(X + Y) &= \\mathbb E\\big((X + Y) - (\\mu_X + \\mu_Y)\\big)^2 \\\\\n  &= \\mathbb E \\big((X - \\mu_X) + (Y - \\mu_Y) \\big)^2 \\\\\n  &= \\mathbb E \\big( (X - \\mu_X)^2 + 2(X - \\mu_X)(Y - \\mu_Y) + (Y - \\mu_Y)^2\\big) \\\\\n  &= \\mathbb E(X - \\mu_X)^2 + 2 \\mathbb E(X - \\mu_X)(Y - \\mu_Y) + \\mathbb E (Y - \\mu_Y)^2 \\\\\n  &= \\Var(X) + 2\\Cov(X,Y) + \\Var(Y) ,\n\\end{align*}\nwhere we used the linearity of expectation.\n\nFor the second part, recall that is $X$ and $Y$ are independent, then $\\Cov(X,Y) = 0$.\n:::\n\n\n## Correlation  {#correlation}\n\nIt can sometimes be useful to \"normalise\" the covariance, by dividing through by the individual standard deviations. This gives a measurement of the linear relationship between two random variables.\n\n::: {.definition}\nLet $X$ and $Y$ be two random variables. Then the **correlation** between $X$ and $Y$ is\n\\[ \\Corr(X,Y) = \\frac{\\Cov(X,Y)} {\\sqrt{\\Var(X)\\,\\Var(Y)}} . \\]\n:::\n\nAs with the sample correlation $r_{xy}$ from Section 1, the correlation is a number between $-1$ and $+1$, where values near $+1$ mean that large values of $X$ and large values of $Y$ are likely to occur together, while values near $-1$ mean that large values of $X$ and small values of $Y$ are likely to occur together.\n\nRecall that, if $X$ and $Y$ are independent, then $\\Cov(X,Y) = 0$. Hence it follows that if $X$ and $Y$ are independent, then $\\Corr(X,Y) = 0$ also.\n\n::: {.example}\nFor the coin-tossing again, we have\n\\[ \\Corr(X,Y) = \\frac{\\Cov(X,Y)} {\\sqrt{\\Var(X)\\,\\Var(Y)}} = \\frac{\\frac12}{\\sqrt{\\frac12 \\times \\frac34}} = \\sqrt{\\tfrac23} = 0.816 .    \\]\n:::\n\n\n\n## Summary  {#summary-L14 .unnumbered}\n\n::: {.mysummary}\n* $\\mathbb E(X + Y) = \\mathbb EX + \\mathbb EY$\n* The covariance is $\\Cov(X,Y) = \\mathbb E(X - \\mu_X)(Y - \\mu_Y) = \\mathbb EXY - \\mu_X \\,\\mu_Y$.\n* $\\Var(X + Y) = \\Var(X) + 2\\Cov(X,Y) + \\Var(Y)$; or if $X$ and $Y$ are independent, then $\\Var(X + Y) = \\Var(X) + \\Var(Y)$.\n:::"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L14-covariance.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo","toc-title":"MATH1710"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L14-covariance.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}