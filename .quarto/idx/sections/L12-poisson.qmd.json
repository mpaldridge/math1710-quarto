{"title":"Poisson distribution","markdown":{"headingText":"Poisson distribution","headingAttr":{"id":"L12-poisson","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nWe have seen three important families of discrete random variables: the Bernoulli, binomial, and geometric distributions. We now look at our fourth and final discrete distribution: the Poisson distribution.\n\n## Definition and properties  {#poisson}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/xbWQalpr5h0\"></iframe>\n:::\n::::\n-->\n\nAnother important distribution is the Poisson distribution. The Poisson distribution (roughly \"*pwa*-song\") is typically used to model \"the number of times something happens in a set period of time\". For example, the number of emails you receive in a day; the number of claims at an insurance company each year; or the number of calls to call centre in one hour. (Famously, one of the first historical datasets modelled using a Poisson distribution was \"the number of Prussian soldiers in different cavalry units kicked to death by their own horse between 1875 and 1894\".) We'll explain why the Poisson distribution is a good model for this in the next subsection.\n\n::: {.definition}\nLet $X$ be a discrete random variable with range $\\{0,1,2,\\dots\\}$ and PMF\n\\[ p_X(x) = \\mathrm e^{-\\lambda}  \\frac{\\lambda^x}{x!} . \\]\nThen we say that $X$ follows the **Poisson distribution** with **rate** $\\lambda$, and write $X \\sim \\text{Po}(\\lambda)$.\n:::\n\nHere, $\\lambda$ is a lower-case Greek letter \"lambda\". I should also note that we interpret $0! = 1$, so\n\\[ p(0) = \\mathrm e^{-\\lambda}  \\frac{\\lambda^0}{0!} = \\mathrm e^{-\\lambda}  \\frac{1}{1} = \\mathrm e^{-\\lambda} . \\]\n\n```{r po-pic, cache = TRUE, echo = FALSE}\nx <- 0:10\n\nplot(x-0.05,   dpois(x, 1.2), type = \"h\", lwd = 4, col = \"blue\", ylim = c(0, 0.4), xlab = \"x\", ylab = \"probability mass function p(x)\")\npoints(x+0.05, dpois(x, 3), type = \"h\", lwd = 4, col = \"red\")\nlegend(\"topright\", c(\"Po(1.2)\", \"Po(3)\"), col = c(\"blue\", \"red\"), lwd = 4)\n```\n\nThe Poisson distribution is named after the French mathematician [Siméon-Denis Poisson](https://mathshistory.st-andrews.ac.uk/Biographies/Poisson/) who wrote about it in 1837, although the origin of the idea is more than 100 years earlier with another French mathematician, [Abraham de Moivre](https://mathshistory.st-andrews.ac.uk/Biographies/De_Moivre/).\n\n<!--\n::: {.example}\n*An insurance company receives large insurance claims of over £100,000 at a rate of $\\lambda = 1.2$ per day, modelled as a Poisson distribution and independent between days. What's the probability that in a week (5 days) they get at least one large claim every day?*\n\nLet $X \\sim \\text{Po}(\\lambda)$ be the number of large claims received in a day. Then the probability there is at least one claim in a day -- which is an \"at least one\" question, suggesting we look at the complement -- is\n\\[ \\mathbb P(X \\geq 1) = 1 - \\mathbb P(X = 0) = 1 - \\mathrm e^{-1.2} \\frac{1.2^0}{0!} = 1 - \\mathrm e^{-1.2} =0.699 .    \\]\n\nSince it's assumed that days are independent, the probability there is at least one large claim all 5 days is\n\\[  \\mathbb P(X \\geq 1)^5 = 0.699^5 = 0.167, \\]\nor about 17%.\n:::\n-->\n\n::: {.example}\n*I receive emails from students at the rate of $\\lambda = 3$ per hour, modelled as a Poisson distribution. What is the probability I get (a) two email in an hour, (b) no email in an hour?*\n\nThe number of emails per hour is $X \\sim \\mathrm{Po}(3)$.\n\nFor (a), we have\n\\[ \\mathbb P(X = 3) = p(3) = \\mathrm{e}^{-3} \\frac{3^2}{2!} = \\frac94 \\mathrm{e}^{-3} = 0.224 . \\]\n\nFor part (b), and remembering that $0! = 1$, we have\n\\[ \\mathbb P(X = 3) = p(0) = \\mathrm{e}^{-3} \\frac{3^0}{0!} = \\mathrm{e}^{-3} = 0.050 . \\]\n:::\n\nThe parameter $\\lambda$ is called the \"rate\" because that indeed the number of emails (or insurance claims, or phone calls, or deaths by horse-kicking) that we expect to see.\n\n::: {.theorem}\nLet $X \\sim \\text{Po}(\\lambda)$. Then\n\n1. $p(x)$ is indeed a PMF, in that $\\displaystyle\\sum_{x=0}^\\infty p(x) = 1$.\n2. $\\mathbb EX = \\lambda$,\n3. $\\Var(X) = \\lambda$.\n:::\n\n::: {.proof}\nWe'll do the first two here, then you can do the variance in [Problem Sheet 4](#P4-long). \n\nIt will be useful to remember the Taylor series for the exponential function,\n\\[ \\mathrm e^\\lambda = \\sum_{x=0}^\\infty \\frac{\\lambda^k}{x!} . \\]\n\nFor part 1, to see that the PMF does indeed sum to one, note that the Taylor series gives us\n\\[ \\sum_{x=0}^\\infty p(x) = \\sum_{x=0}^\\infty \\mathrm e^{-\\lambda} \\frac{\\lambda^x}{x!}\n= \\mathrm e^{-\\lambda} \\sum_{x=0}^\\infty  \\frac{\\lambda^x}{x!} = \\mathrm e^{-\\lambda}\\,\\mathrm e^{\\lambda} = 1. \\]\n\nFor part 2, for the expectation, we have\n\\begin{align*}\n\\mathbb EX &= \\sum_{x=0}^\\infty x\\,\\mathrm e^{-\\lambda}  \\frac{\\lambda^x}{x!} \\\\\n  &= \\mathrm e^{-\\lambda} \\sum_{x=1}^\\infty x\\,\\frac{\\lambda^x}{x!} \\\\\n  &= \\mathrm e^{-\\lambda} \\sum_{x=1}^\\infty \\frac{\\lambda^x}{(x-1)!} \\\\\n  &= \\lambda \\mathrm e^{-\\lambda} \\sum_{x=1}^\\infty \\frac{\\lambda^{x-1}}{(x-1)!}\n\\end{align*}\nIn the second line, we took $\\mathrm e^{-\\lambda}$ outside the sum, and allowed ourselves to start the sum from 1, since the $x = 0$ term was 0 anyway; in the third line, we cancelled the $x$ from the $x!$ to get $(x-1)!$; and in the fourth line we took one of the $\\lambda$s in $\\lambda^x$ outside the sum, to give ourselves terms in $x - 1$ inside the sum. We can now \"re-index\" the sum by putting $y = x - 1$, to get\n\\[ \\mathbb EX = \\lambda \\mathrm e^{-\\lambda} \\sum_{y=0}^\\infty \\frac{\\lambda^{y}}{y!}\n= \\lambda \\mathrm e^{-\\lambda} \\mathrm e^{\\lambda} = \\lambda , \\]\nwhere we used the Taylor series again.\n:::\n\n::: {.mysummary}\nAt this point in the lecture, we took a break to fill in [the **mid-semester check-in survey**](https://forms.office.com/Pages/ResponsePage.aspx?id=qO3qvR3IzkWGPlIypTW3ywARQdZlKXRHsLcXi_ngX8NUNkoxWUlTSTBXQUVSUThGU0pXOTMwTjA0UC4u). This is open for the rest of the week and is anonymous. Written comments in answer to the last question are particular useful -- I will read them all and report back next week on changes I am making to the module in response to your comments.\n:::\n\n## Poisson approximation to the binomial  {#poisson-approx}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/5Ma4YIF_r7w\"></iframe>\n:::\n::::\n-->\n\nSuppose I own a watch shop in Leeds. My watches are very expensive, so I don't need to sell many each day -- in fact, I sell an average of 4.8 watches per day. How should I model the number of watches sold each day as a random variable?\n\nOne way could be to say this. There are $n$ people living in Leeds or nearby, and, on any given day, each of them will independently buy a watch from my shop with some probability $p$. Thus the total number of watches I sell could be modelled as a binomial distribution $\\text{Bin}(n, p)$.\n\nBut what should $n$ and $p$ be? To make the average $\\mathbb EX = np = 4.8$, I must take $p = 4.8/n$. But what about $n$? We know $n$ is a very big number, because Leeds is a big city, so let's take a limit as $n \\to \\infty$. It turns out, that this distribution $\\text{Bin}(n, 4.8/n)$ becomes a Poisson(4.8) distribution!\n\n:::  {.theorem #po-bint}\nFix $\\lambda \\geq 0$, and let $X_n \\sim \\text{Bin}(n, \\lambda/n)$ for all integers $n \\geq \\lambda$. Then $X_n \\to \\text{Po}(\\lambda)$ in distribution as $n \\to \\infty$, by which we mean that if $Y \\sim \\text{Po}(\\lambda)$, then\n\\[ p_{X_n}(x) \\to p_Y(x) \\qquad \\text{for all $x \\in \\{0, 1, \\dots \\}$}. \\]\n:::\n\nA looser way to state the principle of this theorem would be this: *When $n$ is very large and $p$ very small, in such a way that $np$ is a small-ish number, then $\\text{Bin}(n,p)$ is well approximated by $\\text{Po}(\\lambda)$ where $\\lambda = np$.*\n\nThis is why a Poisson distribution is a good model for the number of occurrences in a set time period. It applies if there lots of things that could happen (large $n$), each one is individually unlikely (small $p$), and on average a few of them will actually happen ($\\lambda = np$ small-ish).\n\n::: {.example}\n*A lecturer teaches a module with $n = 100$ students, and estimates that each student turns up to office hours drop-in sessions independently with probability $p = 0.035$. What is the probability that (a) exactly 5, (b) 2 or more students turn up to a drop-in session?*\n\nIf we let $X$ be the  number of students that turn up to a drop-in session, then the exact distribution of $X$ is $X \\sim \\text{Bin}(100, 0.035)$.\n\nFor part (a), we then have\n\\[ \\mathbb P(X = 5) = \\binom{100}{5} 0.035^5 (1 - 0.035)^{100-5} = 0.134 .  \\]\n\nFor part (b), we have an \"at least\" event, so we use the complement rule to get\n\\begin{align*}\n\\mathbb P(X \\geq 2)\n&= 1 - \\mathbb P(X = 0) - \\mathbb P(X = 1) \\\\\n&= 1 - \\binom{100}{0} 0.035^0 (1 - 0.035)^{100-0} + \\binom{100}{1} 0.035^1 (1 - 0.035)^{100 - 1} \\\\\n&= 1 - (1 - 0.035)^{100} + 100 \\times 0.035 (1 - 0.035)^{99} \\\\\n&= 1 - 0.028 - 0.103 \\\\\n&= 0.869\n\\end{align*}\n\nAlternatively, it might be more convenient to approximate $X$ by a Poisson distribution $Y \\sim \\text{Po}(100 \\times 0.035) = \\text{Po}(3.5)$.\n\nFor part (a), this gives\n\\[ \\mathbb P(Y = 5) = \\mathrm e^{-3.5} \\frac{3.5^5}{5!} = 0.132 ,  \\]\nwhich is very close to the exact answer above of $0.134$.\n\nFor part (b), the approximation gives\n\\begin{align*}\n\\mathbb P(Y \\geq 2)\n&= 1 - \\mathbb P(Y = 0) - \\mathbb P(Y = 1) \\\\\n&= 1 - \\mathrm e^{-3.5} \\frac{3.5^0}{0!} - \\mathrm e^{-3.5} \\frac{3.5^1}{1!} \\\\\n&= 1 - \\mathrm e^{-3.5} - 3.5 \\mathrm e^{-3.5} \\\\\n&= 1 - 0.030 - 0.106 \\\\\n&= 0.864\n\\end{align*}\nwhich is very close to the exact answer above of $0.869$.\n\nThe following graph shows how close the $\\text{Po}(3.5)$ distribution is to a $\\text{Bin}(100, 0.035)$ distribution -- not exact, but pretty good.\n\n```{r po-binom-pic, cache = TRUE, echo = FALSE}\nx <- 0:10\n\nplot(x-0.05,   dpois(x, 3.5), type = \"h\", lwd = 4, col = \"blue\", ylim = c(0, 0.25), xlab = \"x\", ylab = \"probability mass function p(x)\")\npoints(x+0.05, dbinom(x, 100, 0.035), type = \"h\", lwd = 4, col = \"red\")\nlegend(\"topright\", c(\"Po(3.5)\", \"Bin(100, 0.035)\"), col = c(\"blue\", \"red\"), lwd = 4)\n```\n:::\n\n\n\nFor completeness, we include a proof of Theorem \\@ref(thm:po-bint) here, although since it discusses use of limits, it's not examinable material for this module.\n\n::: {.proof}\n*(Non-examinable)*\nWe need to show that, as $n \\to \\infty$, \n\\[ p_X(x) = \\binom nx \\left(\\frac{\\lambda}{n}\\right)^x \\left(1 - \\frac{\\lambda}{n}\\right)^{n-x}\n\\to \\mathrm{e}^{-\\lambda} \\frac{\\lambda^x}{x!} = p_Y(x) . \\]\nLet's try. The left-hand side is, by some simple rearrangements,\n\\begin{align*}\n\\binom nx &\\left(\\frac{\\lambda}{n}\\right)^x \\left(1 - \\frac{\\lambda}{n}\\right)^{n-x} \\\\\n  &= \\frac{n(n-1)\\cdots(n-x+1)}{x!} \\frac{\\lambda^x}{n^x} \\left(1 - \\frac{\\lambda}{n}\\right)^{n}\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} \\\\\n  &= \\frac{\\lambda^x}{x!} \\frac{n(n-1)\\cdots(n-x+1)}{n^x} \\left(1 - \\frac{\\lambda}{n}\\right)^{n}\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} \\\\\n  &= \\frac{\\lambda^x}{x!} \\frac{n}{n} \\frac{n-1}{n} \\cdots \\frac{n-x+1}{n} \\left(1 - \\frac{\\lambda}{n}\\right)^{n}\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} \\\\\n  &= \\frac{\\lambda^x}{x!} 1 \\left(1 - \\frac{1}{n}\\right) \\cdots \\left(1 - \\frac{x-1}{n}\\right)  \\left(1 - \\frac{\\lambda}{n}\\right)^{n}\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} .\n\\end{align*}\n\nNow let's take each of the terms in turn. First $\\lambda^x / x!$ looks very promising, and can stay. Second, each of the terms $1, 1 - 1/n, \\dots, 1 - (x-1)/n$ tend to 1 as $n \\to \\infty$. Third, \n\\[ \\left(1 - \\frac{\\lambda}{n}\\right)^{n} \\to \\mathrm{e}^{-\\lambda} ; \\]\nthis is from the standard \"compound interest\" result that\n\\[ \\left(1 + \\frac{a}{n}\\right)^{n} \\to \\mathrm{e}^{a} \\qquad \\text{as $n \\to \\infty$}. \\]\nFinally\n\\[\\left(1 - \\frac{\\lambda}{n}\\right)^{-x} \\to 1 , \\]\nas $1 - \\lambda/n \\to 1$, and $x$ is fixed. Putting all that together gives the result.\n:::\n\n## Poisson process  {#poisson-process}\n\nSuppose an insurance company's call centre is open 10 hours a day, 5 days a week. The call centre receives a \"large claim\" -- a claim in excess of £100,000 -- on average 0.2 times per hour. It seems reasonable, therefore, to model the number of large claims in an hour as a $\\mathrm{Po}(\\lambda)$ distribution with $\\lambda = 0.2$.\n\nHow should we model the nuber of large claims in a day? If the call centre receives $\\lambda = 0.2$ claims per hour, on average, then it receives $10\\lambda = 2$ claims per 10 hours, or one day. It seems reasonable to model this with a $\\mathrm{Po}(10\\lambda)$ distribution.\n\nFurther, the number of claims on one day and on the next day seems like they should be independent.\n\nThe two ideas in the model above lead to what is called the \"Poisson process\".\n\n::: {.definition}\nA random set of arrivals is said to follow a **Poisson process** with rate $\\lambda$ if\n\n1. The number of arrivals in a time period of length $t$ is $\\mathrm{Po}(\\lambda t)$.\n2. The number of arrivals in two non-overlapping time periods are independent.\n:::\n\n::: {.example}\n*Suppose the number of large claims, as discussed above, is modelled as a Poisson process with rate $\\lambda = 0.2$ claims per hour. What is the probability the call centre receives at least one large claim every day this week?*\n\nThe number of large claims in one 10-hour day is $X \\sim \\mathrm{Po}(10\\lambda) = \\mathrm{Po}(2)$. So the probability of getting at least one claim in a day is\n\\[ \\mathbb P(X \\geq 1) = 1 - \\mathbb P(X = 0) = 1 - \\mathrm{e}^{-2} = 0.865 . \\]\n\nBecause the number of claims in each of the five days this week are independent, the probability of getting at least one claim in all five days is\n\\[ \\mathbb P(X \\geq 1)^5 = 0.865^5 = 0.483 . \\]\n:::\n\nThis is just a brief taster of the Poisson process. The Poisson process is studied in much more detail in the second-year module [MATH2750 Introduction to Markov Processes](https://mpaldridge.github.io/math2750/).\n\n\n## Summary  {#summary-06 .unnumbered}\n\n| Distribution | Range | PMF | Expectation | Variance |\n|:----|:-:|:-:|:-:|:-:|\n| **Bernoulli:** $\\text{Bern}(p)$ | $\\{0,1\\}$ | $p(0) = 1- p$, $p(1) = p$ | $p$ | $p(1-p)$ |\n| **Binomial:** $\\text{Bin}(n,p)$ | $\\{0,1,\\dots,n\\}$ | $\\displaystyle\\binom{n}{x} p^x (1-p)^{n-x}$ | $np$ | $np(1-p)$ |\n| **Geometric:** $\\text{Geom}(p)$ | $\\{1,2,\\dots\\}$ | $(1-p)^{x-1}p$ | $\\displaystyle\\frac{1}{p}$ | $\\displaystyle\\frac{1-p}{p^2}$ |\n| **Poisson:** $\\text{Po}(\\lambda)$ | $\\{0,1,\\dots\\}$ | $\\mathrm{e}^{-\\lambda} \\displaystyle\\frac{\\lambda^x}{x!}$ | $\\lambda$ | $\\lambda$ |"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L12-poisson.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo","toc-title":"MATH1710"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L12-poisson.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}