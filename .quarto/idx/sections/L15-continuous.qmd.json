{"title":"Continuous random variables","markdown":{"headingText":"Continuous random variables ","headingAttr":{"id":"L15-continuous","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## What is a continuous random variable?  {#continuous-rv}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/1Oae0a12P1M\"></iframe>\n:::\n::::\n-->\n\n\n\nIn the previous six lectures, we have looked at discrete random variables, whose range is a finite or countably infinite set of separate discrete values. Discrete random variables can be used as a model for \"count data\".\n\nIn this section and the next, we will instead look at continuous random variables, whose range is an uncountable set, a continuum of gradually varying values. Continuous random variables can be used as a model for \"measurement data\". For example:\n\n* The assets of a bank at the end of this year could be modelled as a continuous random variable with range the real numbers $\\mathbb R$, where positive numbers represent credit and negative numbers represent debt.\n* The amount of time a machine in a factory works for before breaking down could be modelled as a continuous random variable with range the positive real numbers $\\mathbb R_+ = \\{x \\in \\mathbb R : x \\geq 0\\}$.\n* The unemployment rate in the UK next January, as a proportion of the population, could be measured as a continuous random variable with range the interval $[0, 1] = \\{x \\in \\mathbb R : 0 \\leq x \\leq 1\\}$.\n\nImagine firing an arrow at a large target. We could ask \"What's the probability that the arrow exactly hits some point?\" -- but this question is difficult to answer. What do we mean by a point? If we mean a mathematically-idealised infinitesimally small point, then I think we'd have to say that the probability is 0.  What makes more sense is too take a section of the target -- perhaps a small circle in the middle, called the \"bulls-eye\" -- and ask what is the probability that the arrow lands in the area of the bulls-eye. Then we could (at least in theory) answer that question -- a good archer would have quite a high probability of landing the arrow in the bulls-eye, while a poor archer would have a smaller chance.\n\nSimilarly, imagine picking a random real number between 0 and 1. We could ask \"What is the probability that the random number is *exactly* $1/\\sqrt{2} = 0.7071068\\dots$?\" But that probability, if it means anything, must be 0. It makes more sense to take an interval of numbers -- say, $[0.7, 0.8]$, the interval from $0.7$ to $0.8$ -- and ask what the probability is of the random number being in that interval.\n\nThis is how continuous random variables work. The probability a continuous random variable $X$ *exactly* hits some value $x$ is $\\mathbb P(X = x) = 0$. But we *can* find the probability $\\mathbb P(a \\leq X \\leq b)$ that $X$ lies in a certain interval and work with that. \n\n\n## Probability density functions  {#pdf}\n\nWith a continuous random variable, the probability of *exactly* getting any particular outcome $X = x$ is 0. However, we can express the \"intensity\" of probability *around* $x$ by $f_X(x)$, where $f_X$ is called the \"probability density function\". The implied metaphor here is that for discrete random variables, we have probability \"mass\" *at* the point $x$, whereas for continuous random variables, we have a \"density\" of probability *around* $x$.\n\n::: {.definition}\nA random variable $X$ is called a **continuous random variable** if the probability of landing in any interval between $a$ and $b$, for $a \\leq b$, can be written as\n\\[ \\mathbb P(a \\leq X \\leq b) = \\int_a^b f_X(x) \\, \\mathrm{d}x , \\]\nfor some non-negative function $f_X$. The function $f_X$ is called the **probability density function** (or **PDF**).\n:::\n\nIn other words, the probability that $X$ is between $a$ and $b$ is the area under the curve of the PDF $f_X(x)$ between $x = a$ and $x = b$.\n\nAs with PMFs, when it's obvious what random variable we're dealing with, we omit the subscript $X$ on the PDF $f_X$.\n\n::: {.example #unifex}\nLet $X$ be a continuous random variable with PDF\n\\[  f(x) = 1 \\qquad \\text{for $0 \\leq x \\leq 1$} \\]\nand $f(x) = 0$ otherwise. This represents a random number between 0 and 1, where the intensity of the probability is equal across the whole interval. This is known as a **continuous uniform distribution**.\n\n```{r contunif-pdf, cache = TRUE, echo = FALSE}\ncurve(x-x+1, from = 0, to = 1,\n      lwd = 2, col = \"blue\", xlim = c(-1,2), ylim = c(0,2),\n      xlab = \"x\", ylab = \"probability density function f(x)\"\n)\ncurve(x - x, from = -1.5, to = 0, lwd = 2, col = \"blue\", add = TRUE)\ncurve(x - x, from = 1,  to = 2.5, lwd = 2, col = \"blue\", add = TRUE)\nsegments(0, 0, 0, 1, lwd = 1, col = \"blue\", lty = 3)\nsegments(1, 0, 1, 1, lwd = 1, col = \"blue\", lty = 3)\npolygon(c(0.5, 0.5, 0.8, 0.8), c(0, 1, 1, 0), col = \"lightblue1\", border = NA)\ncurve(x-x+1, from = 0, to = 1, lwd = 2, col = \"blue\", add = TRUE)\n```\n\n*What is the probability that $X$ is between 0.5 and 0.8?*\n\nWe can calculate this using the definition above. We have\n\\begin{align*}\n  \\mathbb P(0.5 \\leq X \\leq 0.8) &= \\int_{0.5}^{0.8} f(x) \\, \\mathrm dx \\\\\n    &= \\int_{0.5}^{0.8} 1 \\, \\mathrm dx \\\\\n    &= [x]_{0.5}^{0.8} \\\\\n    &= 0.8 - 0.5 \\\\\n    &= 0.3 .\n\\end{align*}\n:::\n\n::: {.example #pdf2}\nLet $Y$ be a continuous random variable with PDF\n\\[ f(y) = \\begin{cases} y & \\text{for $0 \\leq y \\leq 1$} \\\\\n2-y & \\text{for $1 < y \\leq 2$} \\end{cases} \\]\nand $f(y) = 0$ otherwise. This represents a continuous value between 0 and 2 where the probability intensity is highest in the middle around 1 and is lower at the edges near 0 and 2.\n\n```{r second-pdf, cache = TRUE, echo = FALSE}\ncurve(x+1-1, from = 0, to = 1,\n      lwd = 2, col = \"red\", xlim = c(-1,3), ylim = c(0,2),\n      xlab = \"y\", ylab = \"probability density function f(y)\"\n)\npolygon(c(0.5, 0.5, 1, 1.5, 1.5), c(0, 0.5, 1, 0.5, 0), col = \"mistyrose\", border = NA)\ncurve(x+1-1, from = 0, to = 1, lwd = 2, col = \"red\", add = TRUE)\ncurve(2 - x, from = 1, to = 2, lwd = 2, col = \"red\", add = TRUE)\ncurve(x - x, from = -1.5, to = 0, lwd = 2, col = \"red\", add = TRUE)\ncurve(x - x, from = 2,  to = 3.5, lwd = 2, col = \"red\", add = TRUE)\n\n```\n\n*What is the probability $X$ is between $\\frac12$ and $\\frac32$?*\n\nAs before, we have\n\\[ \\mathbb P\\big( \\tfrac12 \\leq Y \\leq \\tfrac32 \\big) = \\int_{\\frac12}^{\\frac32} f(y) \\, \\mathrm dy .   \\]\nBut this time we have to be careful, because $f(y)$ has different expressions below 1 and above 1. We will split the integral up into two parts based on this, to get\n\\begin{align*}\n\\mathbb P\\big( \\tfrac12 \\leq Y \\leq \\tfrac32 \\big) \n  &= \\int_{\\frac12}^{1} f(y) \\, \\mathrm dy + \\int_{1}^{\\frac32} f(y) \\, \\mathrm dy \\\\\n    &= \\int_{\\frac12}^{1} y \\, \\mathrm dy + \\int_{1}^{\\frac32} (2-y) \\, \\mathrm dy \\\\\n    &= \\left[ \\tfrac12 y^2\\right]_{\\frac12}^1 + \\left[ 2y-\\tfrac12 y^2\\right]_1^{\\frac32} \\\\\n    &= \\tfrac12 - \\tfrac18 + \\big(\\tfrac62 - \\tfrac98\\big) - \\big(2 - \\tfrac12\\big) \\\\\n    &= \\tfrac34 .\n\\end{align*}\n:::\n\n\n## Properties of continuous random variables {#prop-cont}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/J7CtqG0HErc\"></iframe>\n:::\n::::\n-->\n\n\nThe good news is that almost all of the properties we know and love about discrete distributions also follow through for continuous distribution -- except you swap the PMF for the PDF and swap sums for integrals.\n\n|   Discrete random variables   |   Continuous random variables   |\n|-----|-----|\n| A discrete random variable $X$ is defined by a **probability mass function** (PMF) $p(x)$, which represents the probability of getting exactly $x$. | A continuous random variable $X$ is defined by a **probability density function** (PDF) $f(x)$, which represents the intensity of probability around $x$. |\n| The PMF is positive, in that $p(x) \\geq 0$ for all $x$. | The PDF is positive, in that $f(x) \\geq 0$ for all $x$. |\n| The PMF sums to 1, in that \\[ \\sum_{x} p(x) = 1. \\] | The PDF integrates to 1 in that \\[ \\int_{-\\infty}^{\\infty} f(x) \\, \\mathrm{d}x = 1.\\]\n| The **cumulative distribution function** (CDF) is $F(x) = \\mathbb P(X \\leq x)$, and is given by a sum \\[ F(x) = \\sum_{y \\leq x} p(y) .\\] | The **cumulative distribution function** (CDF) is $F(x) = \\mathbb P(X \\leq x)$, and is given by an integral \\[ F(x) = \\int_{-\\infty}^x f(y) \\, \\mathrm{d}y .\\]  |\n| The **expectation** is the sum \\[ \\mathbb EX = \\sum_{x} x\\,p(x) . \\] | The **expectation** is the integral \\[ \\mathbb EX = \\int_{-\\infty}^{\\infty} x\\,f(x)\\,\\mathrm dx . \\] |\n| The expectation of a function $g(X)$ of $X$ is the sum \\[ \\mathbb Eg(X) = \\sum_{x} g(x)\\,p(x) . \\] | The expectation of a function $g(X)$ of $X$ is the integral  \\[ \\mathbb Eg(X) = \\int_{-\\infty}^{\\infty} g(x)\\,f(x)\\,\\mathrm dx . \\] |\n| Linearity of expectation says that \\[ \\mathbb E(aX+b) = a\\mathbb EX + b .\\] |  Linearity of expectation says that \\[ \\mathbb E(aX+b) = a\\mathbb EX + b .\\] |\n| The **variance** is $\\Var(X) = \\mathbb E(X - \\mu)^2$, which also has the computational formula $\\Var(X) = \\mathbb EX^2 - \\mu^2$. | The **variance** is $\\Var(X) = \\mathbb E(X - \\mu)^2$, which also has the computational formula $\\Var(X) = \\mathbb EX^2 - \\mu^2$. |\n\nNote, however, one property that doesn't follow through: Because, for a PMF, $p(x) = \\mathbb P(X = x)$ represented a probability, we had $p(x) \\leq 1$ for all $x$. However, because, for a PDF, $f(x)$ only represents intensity of probability, there's no contradiction to having $f(x) > 1$ (although keeping the integral to 1 means that we can't have $f(x) > 1$ too much). So $f(x) = 10$ for $0 <x < 0.1$ and $f(x) = 0$ otherwise is a perfectly legitimate PDF, for example.\n\n::: {.example}\nLet's return to the case where $X$ be a continuous uniform distribution, with\n\\[  f(x) = 1 \\qquad \\text{for $0 \\leq x \\leq 1$}  \\]\nand $f(x) = 0$ otherwise. Let's go through the properties from the table above.\n\nFirst, it's clear that $f(x) \\geq 0$ for all $x$.\n\nSecond, the PDF does indeed integrate to 1, because\n\\[ \\int_{-\\infty}^\\infty f(x) \\, \\mathrm dx = \\int_0^1 1 \\, \\mathrm dx = [x]_0^1 = 1 .    \\]\nBecause this PDF is zero below 0 and above 1, we only had to integrate between 0 and 1, with the rest of the integral over the real line being 0.\n\nThird, the CDF $F$. It's clear that $F(x) = \\mathbb P(X \\leq x) = 0$ for $x < 0$, and $F(x) = \\mathbb P(X \\leq x) = 1$ for $x > 1$. In between, we have\n\\[ F(x) = \\int_{-\\infty}^x f(y) \\,\\mathrm dy = \\int_0^x 1\\, \\mathrm dy = [y]_0^x = x .   \\]\nSo, altogether, the CDF is\n\\[  F(x) = \\begin{cases} 0 & \\text{for } x < 0 \\\\ x & \\text{for }0 \\leq x \\leq 1 \\\\ 1 & \\text{for }x > 1 . \\end{cases} \\]\n\n```{r contunif-cdf, cache = TRUE, echo = FALSE}\ncurve(x + 1 - 1, from = 0, to = 1,\n      lwd = 2, col = \"blue\", xlim = c(-1,2), ylim = c(0,1),\n      xlab = \"x\", ylab = \"cumulative distribution function F(x)\"\n)\ncurve(x - x, from = -1.5, to = 0, lwd = 2, col = \"blue\", add = TRUE)\ncurve(x - x + 1, from = 1,  to = 2.5, lwd = 2, col = \"blue\", add = TRUE)\n```\n\nFourth, the expectation is\n\\[ \\mathbb EX = \\int_{\\infty}^\\infty x\\,f(x)\\,\\mathrm dx = \\int_0^1 x \\, \\mathrm dx = \\left[\\tfrac12 x^2 \\right]_0^1 = \\tfrac12 - 0 = \\tfrac12 .   \\]\n\nFinally, to calculate the variance using the computational formula $\\Var(X) = \\mathbb EX^2 - \\mu^2$, we first need $\\mathbb EX^2$. This is\n\\[ \\mathbb EX^2 = \\int_{\\infty}^\\infty x^2\\,f(x)\\,\\mathrm dx = \\int_0^1 x^2 \\, \\mathrm dx = \\left[\\tfrac13 x^3 \\right]_0^1 = \\tfrac13 - 0 = \\tfrac13 .   \\]\nSo, the variance is\n\\[ \\Var(X) = \\mathbb EX^2 - \\mu^2 = \\tfrac13 - \\left(\\tfrac12\\right)^2 = \\tfrac13 - \\tfrac14 = \\tfrac{1}{12} . \\]\n:::\n\n::: {.example}\nLet's also return to the \"triangular\" PDF from Example \\@ref(exm:pdf2),\n\\[ f(y) = \\begin{cases} y & \\text{for $0 \\leq y \\leq 1$} \\\\\n2-y & \\text{for $1 < y \\leq 2$} \\end{cases} \\]\nand $f(y) = 0$ otherwise. We'll just do the CDF and the expectation. (You can do the others yourself, if you like.)\n\nFor the CDF, it's clear that $F(y) = 0$ for $y < 0$ and $F(y) = 1$ for $y > 2$. Again, we split the $0 \\leq y \\leq 1$ case and the $1 < y \\leq 2$ case. In the first case, for $0 \\leq y \\leq 1$, we have\n\\begin{align*}\n  F(x) &= \\int_{-\\infty}^y f(z) \\, \\mathrm dz \\\\\n    &= \\int_0^y z \\, \\mathrm dz \\\\\n    &= \\left[ \\tfrac12 z^2 \\right]_0^y \\\\\n    &= \\tfrac 12 y^2 .\n\\end{align*}\nIn the second case, for $1 < y \\leq 2$, we have\n\\begin{align*}\n  F(x) &= \\int_{-\\infty}^y f(z) \\, \\mathrm dz \\\\\n    &= \\int_0^1 z \\, \\mathrm dz + \\int_1^y (2 - z)\\,\\mathrm dz \\\\\n    &= \\left[ \\tfrac12 z^2 \\right]_0^1 + \\left[ 2z - \\tfrac12 z^2 \\right]_1^y \\\\\n    &= \\tfrac 12 - 0 + 2y - \\tfrac12 y^2 - 2 + \\tfrac12 \\\\\n    &= 2y - \\tfrac12 y^2 - 1  .\n\\end{align*}\nHence, the CDF is\n\\[ F(y) = \\begin{cases} 0 & \\text{for $y < 0$} \\\\\n            \\tfrac12 y^2 & \\text{for $0 \\leq y \\leq 1$} \\\\\n            2y - \\tfrac12 y^2 - 1  & \\text{for $1 < y \\leq 2$} \\\\\n            1 & \\text{for $y > 2$}. \\end{cases} \\]\n\n```{r second-cdf, cache = TRUE, echo = FALSE}\ncurve(x^2/2+1-1, from = 0, to = 1,\n      lwd = 2, col = \"red\", xlim = c(-1,3), ylim = c(0,1),\n      xlab = \"y\", ylab = \"cumulative distribution function F(y)\"\n)\ncurve(2*x - 1 - x^2/2, from = 1, to = 2, lwd = 2, col = \"red\", add = TRUE)\ncurve(x - x, from = -1.5, to = 0, lwd = 2, col = \"red\", add = TRUE)\ncurve(x - x+1, from = 2,  to = 3.5, lwd = 2, col = \"red\", add = TRUE)\n```\n\nFor the expectation, we have\n\\begin{align*}\n\\mathbb EY &= \\int_{-\\infty}^{\\infty} y\\, f(y) \\, \\mathrm dy \\\\\n  &= \\int_0^1 y^2 \\mathrm dy + \\int_1^2 y(2 - y)\\, \\mathrm dy \\\\\n  &= \\left[ \\tfrac13 y^3 \\right]_0^1 + \\left[  y^2 - \\tfrac13 y^3 \\right]_1^2 \\\\\n  &= \\tfrac13 - 0 + 4 - \\tfrac83 - 1 + \\tfrac13 \\\\\n  &= 1 .\n\\end{align*}\n:::\n\n\n## Summary  {#summary-L15 .unnumbered}\n\n::: {.mysummary}\n* A continuous random variable is defined by its probability density function $f$, where\n\\[ \\mathbb P(a \\leq X \\leq b) = \\int_a^b f(x) \\, \\mathrm dx . \\]\n* Most properties of discrete random variables hold, with the PMF replaced by the PDF, and sums by integrals.\n* For example, the expectation is $\\mathbb EX = \\displaystyle\\int_{-\\infty}^\\infty x\\, f(x) \\, \\mathrm dx$.\n:::"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L15-continuous.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L15-continuous.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}