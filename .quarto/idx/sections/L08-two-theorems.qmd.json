{"title":"Two theorems on conditional probability","markdown":{"headingText":"Two theorems on conditional probability","headingAttr":{"id":"L08-two-theorems","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nLast time we met the conditional probability $\\mathbb P(B \\mid A)$ of one event $B$ given another event $A$. In this lecture we will be looking two very useful theorems about conditional probability, called the **law of total probability** and **Bayes' theorem** -- and they're particularly powerful when used together.\n\n## Law of total probability  {#total-prob}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/zBfFOYkfoss\"></iframe>\n:::\n::::\n-->\n\n::: {.example #dice-total}\n*My friend has three dice: a 4-sided dice, a 6-side dice, and a 10-side dice. He picks one of them at random, with each dice equally likely. What is the probability my friend rolls a 5?*\n\nIf my friend were to tell which dice he picked, then this question would be very easy! If we write $D_4$, $D_6$ and $D_{10}$ to be the events that he picks the 4-sided, 6-sided, or 10-sided dice, then we know immediately that\n\\[ \\mathbb P(\\text{roll 5} \\mid D_4) = 0 \\qquad \\mathbb P(\\text{roll 5} \\mid D_6) = \\tfrac16 \\qquad \\mathbb P(\\text{roll 5} \\mid D_{10}) = \\tfrac{1}{10} .  \\]\nWhat we need is a way to combine the results for different \"sub-cases\" into an over-all answer.\n:::\n\nLuckily, there exists just such a tool for this job! It's called the \"law of total probability\" (also known as the \"partition theorem\"). The important point is to make sure that the different sub-cases cover all possibilities, but that only one of them happens at a time.\n\n::: {.definition}\nA set of events $A_1, A_2, \\dots, A_n$ are said to be a **partition** of the sample space $\\Omega$ if \n\n1. they are disjoint, in that $A_i \\cap A_j = \\varnothing$ for all $i \\neq j$;\n1. they cover space, in that $A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\Omega$.\n:::\n\n::: {.theorem #thlawtotal name=\"Law of total probability\"}\nLet $A_1, A_2, \\dots, A_n$ be a partition, and $B$ another event. Then\n\\[ \\mathbb P(B) = \\sum_{i=1}^n \\mathbb P(A_i) \\, \\mathbb P(B \\mid A_i) . \\]\n:::\n\nSo the law of total probability tells us we can add up the probabilities $\\mathbb P(B \\mid A_i)$ for each of the sub-cases provided we weight them by how likely $\\mathbb P(A_i)$ by how likely each sub-case is.\n\n::: {.proof}\nSince the partition of $A_i$s cover space, we can split up $B$ depending on which part of the partition it is in:\n\\[  B = (B \\cap A_1) \\cup (B \\cap A_2) \\cup \\cdots \\cup (B \\cap A_n) .  \\]\n\n*[I meant to draw a picture here, but didn't get round to it -- perhaps you'd like to draw your own?]*\n\nSince the $A_i$ are disjoint, the union on the right is disjoint also.\nTherefore we can use Axiom 3 to get\n\\[ \\mathbb P(B) = \\sum_{i=1}^n \\mathbb P(B \\cap A_i) . \\]\nBut using the definition of conditional probability, each \"summand\" (term inside the sum) is\n\\[ \\mathbb P(B \\cap A_i) = \\mathbb P(A_i) \\, \\mathbb P(B \\mid A_i) . \\]\nThe result follows.\n:::\n\n::: {.thpart}\n**Example \\@ref(exm:dice-total) continued.** Returning to our dice example, we see that $\\{D_4, D_6, D_{10}\\}$ is indeed a partition, since my friend must choose exactly one of the three dice. So the law of total probability tells us that\n\\[ \\mathbb P(\\text{roll 5}) = \\mathbb P(D_4) \\, \\mathbb P(\\text{roll 5} \\mid D_4) +  \\mathbb P(D_6) \\, \\mathbb P(\\text{roll 5} \\mid D_6) + \\mathbb P(D_{10}) \\, \\mathbb P(\\text{roll 5} \\mid D_{10}) . \\]\n\nWe were told that all the dice were picked with equal probability, so $\\mathbb P(D_4) = \\mathbb P(D_6) = \\mathbb P(D_{10}) = \\frac13$, and we've already  calculated the individual conditional probabilities as\n\\[ \\mathbb P(\\text{roll 4} \\mid D_4) = 0 \\qquad \\mathbb P(\\text{roll 4} \\mid D_6) = \\tfrac16 \\qquad \\mathbb P(\\text{roll 4} \\mid D_{10}) = \\tfrac{1}{10} .  \\]\nTherefore, we have\n\\[ \\mathbb P(\\text{roll 5}) = \\tfrac13\\times 0 +  \\tfrac13\\times\\tfrac16 +  \\tfrac13\\times\\tfrac1{10} = \\tfrac{8}{90} = 0.089. \\]\n:::\n\n\n## Bayes' theorem  {#bayes}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/uLSewGUPH1g\"></iframe>\n:::\n::::\n-->\n\n\nIn this section, we will discuss an important result called **Bayes' theorem**. \nLet's first state and prove this result, and do an example, and then afterwards we'll talk about two reasons why Bayes' theorem is so important.\n\n::: {.theorem #thbayes name=\"Bayes' theorem\"}\nFor events $A$ and $B$ with $\\mathbb P(A), \\mathbb P(B) > 0$, we have\n\\[ \\mathbb P(A \\mid B) = \\frac{\\mathbb P(A) \\,\\mathbb P(B \\mid A)}{\\mathbb P(B)} .  \\]\n:::\n\nBayes' theorem is thought to have first appeared in the writings of Rev. [Thomas Bayes](https://mathshistory.st-andrews.ac.uk/Biographies/Bayes/), a British church minister and mathematician, shortly after his death, in the 1760s. (Bayes' work was significantly edited by [Richard Price](https://mathshistory.st-andrews.ac.uk/Biographies/Price/), another minister--mathematician, and many historians think that Price deserves a large share of the credit.)\n\n::: {.proof}\nFrom the definition of conditional probability, we can write $\\mathbb P(A \\cap B)$ in two different ways: we can write it as\n\\[  \\mathbb P(A \\cap B) = \\mathbb P(A) \\, \\mathbb P(B\\mid A) , \\]\nbut we can also write it as\n\\[  \\mathbb P(A \\cap B) = \\mathbb P(B) \\, \\mathbb P(A\\mid B) . \\]\nSince these are two different ways of writing the same thing, we can equate them, to get\n\\[ \\mathbb P(A) \\, \\mathbb P(B\\mid A) = \\mathbb P(B) \\, \\mathbb P(A\\mid B) . \\]\nDividing both sides by $\\mathbb P(B)$ gives the result.\n:::\n\n::: {.example}\n*My friend again secretly picks the 4-sided, 6-sided, or 10-sided dice, each with probability $\\frac13$. He rolls that secret dice, and tells me he rolled a 5. What is the probability he picked the 6-sided dice?*\n\nThis is asking us to calculate $\\mathbb P(D_6 \\mid \\text{roll 5})$. Bayes' theorem tells us that\n\\[\n  \\mathbb P(D_6 \\mid \\text{roll 5})\n  = \\frac{\\mathbb P(D_6) \\, \\mathbb P(\\text{roll 5} \\mid D_6)}{\\mathbb P(\\text{roll 5})} \n  = \\frac{\\frac13 \\times \\frac16}{\\frac{8}{90}} \n  = \\tfrac{5}{8} ,\n\\]\nsince we had calculated $\\mathbb P(\\text{roll 5}) = \\frac{8}{90}$ in the previous subsection.\n:::\n\nThe first way to think about Bayes' theorem is that it tells us how to relate $\\mathbb P(A \\mid B)$ and $\\mathbb P(B \\mid A)$. Remember that $\\mathbb P(A \\mid B)$ and $\\mathbb P(B \\mid A)$ are not the same thing! The conditional probability someone is under 40 given they are a Premiership footballer is very high, but the conditional probability someone is a Premiership footballer given they are under 40 is very low.\n\nBayes' theorem, in this first view, is a useful technical result that helps us switch the order of a conditional probability from $B$ given $A$ to $A$ given $B$: we have\n\\[ \\mathbb P(A \\mid B) = \\frac{\\mathbb P(A)}{\\mathbb P(B)} \\times \\mathbb P(B \\mid A) .  \\]\n\nIn the dice example, the probability $\\mathbb P(\\text{roll 5} \\mid D_6) = \\frac16$ was very obvious, but Bayes' theorem allowed us to reverse the conditioning, to find $\\mathbb P(D_6 \\mid \\text{roll 5}) = \\frac58$ instead.\n\nThe second way to think about Bayes' theorem is that it tells us how to update our beliefs as we acquire more evidence. That is, we might start by believing that the probability some event $A$ will occur is $\\mathbb P(A)$. But then we find out that $B$ has occurred, so we want to incorporate this knowledge and update our belief of the probability $A$ will occur to $\\mathbb P(A \\mid B)$, the conditional probability $A$ will occur given this new evidence $B$.\n\nBayes theorem, in this second view, tells us how to update from $\\mathbb P(A)$ to $\\mathbb P(A \\mid B)$: we have\n\\[ \\mathbb P(A \\mid B) = \\mathbb P(A) \\times \\frac{\\mathbb P(B \\mid A)}{\\mathbb P(B)} .  \\]\n\nIn the dice example, we initially believed there was a $\\mathbb P(D_6) = \\frac13 = 0.333$ chance our friend had chosen the six-sided dice. But when we heard that our friend had rolled a 5, we updated our belief to now thinking there was now a $\\mathbb P(D_6 \\mid \\text{roll 5}) =\\frac58 = 0.625$ chance it was the 6-sided dice.\n\nThis second way of thinking about Bayes' theorem is at the heart of **Bayesian statistics**. In Bayesian statistics, we start with a \"prior\" belief about a model, then, after collecting some data, we update, using Bayes' theorem, to a \"posterior\" belief about the model. We will discuss Bayesian statistics much more in Week 10.\n\n\n\n\nQuite often we use Bayes' theorem and the law of total probability together. If we have a partition $A_1, A_2, \\dots, A_n$, perhaps representing some possible hypotheses, and we observe some evidence $B$, then Bayes' theorem tells us how likely each hypothesis is given the evidence:\n\\[ \\mathbb P(A_i \\mid B) = \\frac{\\mathbb P(A_i) \\,\\mathbb P(B \\mid A_i)}{\\mathbb P(B)} .  \\]\nBut this shared denominator $\\mathbb P(B)$ can be expanded using the law of total probability\n\\[ \\mathbb P(B) = \\sum_{j=1}^n \\mathbb P(A_j) \\,\\mathbb P(B \\mid A_j) . \\]\nPutting these together, we get the following.\n\n::: {.theorem #bayes-total}\nLet $\\{A_1, A_2, \\dots, A_n\\}$ be a partition of a sample space and let $B$ be another event. Then, for all $i=1,2,\\dots,n$, we have\n\\[ \\mathbb P(A_i \\mid B) = \\frac{\\mathbb P(A_i) \\,\\mathbb P(B \\mid A_i)}{\\sum_{j=1}^n \\mathbb P(A_j) \\, \\mathbb P(B \\mid A_j)} .  \\]\n:::\n\nThis is essentially what we did with the dice example -- although we split up the calculation into two separate parts rather than using this formula directly.\n\n\n## Diagnostic testing  {#screening}\n\n::: {.example}\n*Members of the public are tested for a certain rare disease. About 2% of the population have the disease. The test is 95% accurate, in the following sense: if you have the disease, there's a 95% chance you correctly get a positive test result; while if you don't have the disease, there's a 95% chance you correctly get a negative test result. Suppose you get a positive test result. What is the probability you have the disease?*\n\nThe first thing we have to do is translate the words in the question into probability statements. Let $D$ be the event you have the disease, so $D^\\comp$ is the event you don't have the disease, and let $+$ be the event you get a positive result. Then the question tells us that\n\n* $\\mathbb P(D) = 0.02$ and $\\mathbb P(D^\\comp) = 0.98$;\n* $\\mathbb P({+} \\mid D) = 0.95$;\n* $\\mathbb P({+}\\mid D^\\comp) = 0.05$;\n* we want to find $\\mathbb P(D \\mid {+})$.\n\nImportantly, $D$ (you have the disease) and $D^\\comp$ (you don't) make up a partition. So Theorem \\@ref(thm:bayes-total) tells us that\n\\[  \\mathbb P(D \\mid {+}) = \\frac{\\mathbb P(D) \\,\\mathbb P({+} \\mid D)}{\\mathbb P(D) \\,\\mathbb P({+} \\mid D)+\\mathbb P(D^\\comp) \\,\\mathbb P({+} \\mid D^\\comp)} . \\]\nPutting in all the numbers we have, we get\n\\[ \\mathbb P(D \\mid {+}) = \\frac{0.02 \\times 0.95}{0.02 \\times 0.95 + 0.98 \\times 0.05} = 0.28 .\\]\n\nSo if you get a positive result on this 95%-accurate test, there's still only about a 1 in 4 chance you actually have the disease.\n:::\n\nMany people find this result surprising. It sometimes helps to put more concrete numbers on things. Suppose 1000 people get tested. On average, we expect about 20 of them to have the disease, and 980 of to not have the disease. Of the 20 with the disease, on average 19 will correctly test positive, while 1 will test negative. Of the 980 without the disease, an average 931 will correctly test negative, but 49 will wrongly test positive. So of the $19+49 = 68$ people with positive tests, only 19 of them actually have the disease, which is 28%.\n\nThe key point is that the disease is rare -- only 2% of people have it. So even though positive test increases the likelihood you have the disease a lot (it's about 14 times more likely), it's not enough to make it a very large probability.\n\n## Summary  {#summary-L08 .unnumbered}\n\n::: {.mysummary}\n* The law of total probability says that if $A_1, A_2, \\dots A_n$ is a partition of the sample space (that is, exactly one of them occurs), then\n\\[ \\mathbb P(B) = \\sum_{i=1}^n \\mathbb P(A_i) \\, \\mathbb P(B \\mid A_i) . \\]\n* Bayes' theorem says that ${\\displaystyle \\mathbb P(A \\mid B) = \\frac{\\mathbb P(A) \\,\\mathbb P(B \\mid A)}{\\mathbb P(B)} }$.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L08-two-theorems.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo","toc-title":"MATH1710"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L08-two-theorems.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}