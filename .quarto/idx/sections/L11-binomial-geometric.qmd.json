{"title":"Binomial and geometric distributions","markdown":{"headingText":"Binomial and geometric distributions ","headingAttr":{"id":"L11-binomial-poisson","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\nLast week, we developed the idea of random variables, and in particular discrete random variables. We saw that the benefit of random variables is that we can just worry about their distribution, which often allows us to move the sample space $\\Omega$ and other more technical matters into the background. (Here, we informally use the word \"distribution\" to refer to the probability mass function of a random variable -- or, later, the continuous equivalent, the probability density function).\n\nThere are some distributions -- or, rather, some families of distributions -- that are so useful that we often want to use them for modelling real-world quantities. This week, we will look at a number of useful discrete distributions.\n\n## Binomial distribution  {#binomial}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/x6pVSCBjuzk\"></iframe>\n:::\n::::\n-->\n\nOne family of distributions we have already seen is the Bernoulli trial $\\text{Bern}(p)$, which is 1 with probability $p$ and 0 with probability $1-p$. We saw that this could model whether or a biased coin lands Heads, or more generally whether an experiment is successful.\n\n::: {.example}\n*Suppose we toss 10 independent biased coins, each of which lands Heads with probability 0.7 and Tails with probability 0.3. What is the probability we get exactly 8 Heads altogether?*\n\nThe probability that any specific 8 coins land Heads and the other 2 land Tails is $0.7^8\\times 0.3^2$. However, there are $\\binom{10}{8}$ choices for which 8 coins are the ones that land Heads. Hence, the probability is\n\\[ \\mathbb P(\\text{8 Heads}) = \\binom{10}{8} \\times 0.7^8 \\times 0.3^2 = 0.23.\\]\n:::\n\nThis is a special case of the binomial distribution.\n\n::: {.definition}\nLet $X$ be a discrete random variable with range $\\{0,1,2,\\dots,n\\}$ and PMF\n\\[ p(x) = \\binom{n}{x} p^x (1-p)^{n-x} . \\]\nThen we say that $X$ follows the **binomial distribution** with parameters $n$ and $k$, and write $X \\sim \\text{Bin}(n,p)$.\n:::\n\nSo a binomial random variable represents the number of successes in $n$ Bernoulli trials. In our previous example, the number of Heads from the coin tosses was $\\text{Bin}(10, 0.7)$.\n\n\n```{r binom-pic, cache = TRUE, echo = FALSE}\nx <- 0:10\n\nplot(x-0.05,   dbinom(x, 10, 0.4), type = \"h\", lwd = 4, col = \"blue\", ylim = c(0, 0.3), xlab = \"x\", ylab = \"probability mass function p(x)\")\npoints(x+0.05, dbinom(x, 10, 0.8), type = \"h\", lwd = 4, col = \"red\")\nlegend(\"topleft\", c(\"Bin(10, 0.4)\", \"Bin(10, 0.8)\"), col = c(\"blue\", \"red\"), lwd = 4)\n```\n\n::: {.example}\n*Let $X \\sim \\mathrm{Bin}(8, 0.2)$. What is (a) $\\mathbb P(X = 3)$? (b) $\\mathbb P(X \\geq 2)$?*\n\nFor (a), we have from the definition\n\\[ \\mathbb P(X = 3) = \\binom83 0.2^3 (1 - 0.2)^{8-3} = 56\\times 0.2^3\\times0.8^5 = 0.147 .\\]\n\nFor (b), this is an \"at least\" question, so it's more convenient to look at the complementary event, $\\mathbb P(X < 2)$. So\n\\begin{align*}\n\\mathbb P(X \\geq 2) &= 1 - \\mathbb P(X < 2) \\\\\n  &= 1 - \\mathbb P(X = 0) - \\mathbb P(X = 1) \\\\\n  &= 1 - 0.8^8 - 8\\times 0.2 \\times 0.8^7 \\\\\n  &= 1 - 0.168 - 0.336 \\\\\n  & = 0.497 .\n\\end{align*}\n:::\n\nWhat about the expectation and variance of a binomial random variable?\n\n::: {.theorem}\nLet $X \\sim \\text{Bin}(n, p)$. Then\n\n* $\\mathbb EX = np$,\n* $\\Var(X) =np(1-p)$.\n:::\n\nOne can prove this by working out the sums -- for example, the expectation is the value of the sum\n\\[ \\mathbb EX = \\sum_{x=0}^n x \\binom{n}{x} p^x (1-p)^{n-x} , \\]\nwhich is a bit tricky to calculate, but not fundamentally difficult mathematics.\nHowever, in next section we will see an easier way, so we'll reserve the proof until then instead.\n\nFor my 10 biased coins that are each Heads with probability $0.7$, the expectation and variance are\n\\begin{align*}\n  \\mathbb EX &= 10 \\times 0.7 = 7 \\\\\n  \\Var(X) &= 10 \\times 0.7 \\times 0.3 = 2.1\n\\end{align*}\n\n\n\n## Geometric distribution  {#geometric}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/kQsgZ-kF6PY\"></iframe>\n:::\n::::\n-->\n\n::: {.example}\n*I decide to roll a fair dice until I first roll a six, and then stop. What's the probability I get the first six on my 5th roll of the dice?*\n\nFor the first six to be on the 5th attempt, the first 4 rolls have to be non-sixes, and then the fifth roll has to be a six. This has probability\n\\[ \\left(\\tfrac56\\right)^4 \\times  \\tfrac16 = \\tfrac{625}{7776} = 0.08.\\]\n:::\n\nThis is a special case of the geometric distribution.\n\n::: {.definition}\nLet $X$ be a discrete random variable with range $\\{1,2,\\dots\\}$ and PMF\n\\[ p(x) = (1-p)^{x-1}p . \\]\nThen we say that $X$ follows the **geometric distribution** with parameter $p$, and write $X \\sim \\text{Geom}(p)$.\n:::\n\nSo a geometric random variable represents the number of Bernoulli$(p)$ trials until the first success. In our previous example, the number of dice rolls until a six was $\\text{Geom}(\\frac16)$.\n\n\n```{r geom-pic, cache = TRUE, echo = FALSE}\nx <- 0:14\n\nplot(x+1-0.1,   dgeom(x, 0.2), type = \"h\", lwd = 4, col = \"blue\", xlim = c(0, 15), ylim = c(0, 0.4), xlab = \"x\", ylab = \"probability mass function p(x)\")\npoints(x+1+0.1, dgeom(x, 0.4), type = \"h\", lwd = 4, col = \"red\")\nlegend(\"topright\", c(\"Geom(0.2)\", \"Geom(0.4)\"), col = c(\"blue\", \"red\"), lwd = 4)\n```\n\n::: {.example}\n*Let $X \\sim \\mathrm{Geom}(0.4)$. What is (a) $\\mathbb P(X = 3)$? (b) $\\mathbb P(X \\geq 3)$.*\n\nFor part (a), we have\n\\[ \\mathbb P(X = 3) = (1 - 0.4)^2 \\times 0.4 = 0.144 . \\]\n\nFor part (b), we have\n\\[ \\mathbb P(X \\geq 3) = 1 - \\mathbb P(X =1) - \\mathbb P(X = 2) = 1 - 0.4 - (1-0.4)\\times 0.4 = 1- 0.64 = 0.36 . \\]\n:::\n\n\n::: {.theorem}\nLet $X \\sim \\text{Geom}(p)$. Then\n\n* $\\mathbb EX = \\displaystyle\\frac1p$,\n* $\\Var(X) = \\displaystyle\\frac{1-p}{p^2}$.\n:::\n\nSo the expected number of rolls until rolling a six is\n\\[ \\mathbb EX = \\frac{1}{\\frac16} = 6 , \\]\nwith variance\n\\[ \\Var(X) = \\frac{1 - \\frac16}{\\big(\\frac16\\big)^2} = 30 . \\]\n\n::: {.proof}\n*(Non-examinable)*\nFor the expectation, we want to calculate\n\\[ \\mathbb EX = \\sum_{x=1}^\\infty x (1-p)^{x-1} p = p \\sum_{x=0}^\\infty x (1-p)^{x-1}. \\]\n(We can include the $x = 0$ term in the sum since it is equal to 0.)\n\nAt this point we will invoke the identity\n\\[ \\sum_{x = 0}^\\infty x a^{x-1} = \\frac{1}{(1-a)^2} , \\]\nwhich can be proved by differentiating the standard sum of a geometric progression\n\\[ \\sum_{x = 0}^\\infty a^x = \\frac{1}{1 - a} \\]\nwith respect to $a$.\n\nUsing that identity with $a = 1-p$, we get\n\\[ \\mathbb EX = p \\sum_{x=0}^\\infty x (1-p)^{x-1} = p\\, \\frac{1}{\\big(1 - (1-p)\\big)^2} = \\frac{1}{p} , \\]\nas required.\n\nFor the variance, we will use a trick that sometimes comes in useful, which is to start by calculating $\\mathbb EX(X-1)$. Here we get\n\\[ \\mathbb EX(X-1) = \\sum_{x=1}^\\infty x (x-1) (1-p)^{x-1} p = p(1-p) \\sum_{x=0}^\\infty x(x-1) (1-p)^{x-2} . \\]\nTo calculate the sum, we note that differentiating the geometric progression formula twice gives\n\\[ \\sum_{x = 0}^\\infty x(x-1) a^{x-2} = \\frac{2}{(1-a)^3} , \\]\nso we get\n\\[ \\mathbb EX(X-1) = p(1-p) \\sum_{x=0}^\\infty x(x-1) (1-p)^{x-2} = p(1 -p) \\, \\frac{2}{p^3} = \\frac{2(1-p)}{p^2} . \\]\n\nWe now want to use the computational formula $\\Var(X) = \\mathbb EX^2 - \\mu^2$ to get the variance. We know $\\mu = 1/p$, and from the calculation above, we have\n\\[ \\mathbb EX(X-1) = \\mathbb EX^2 - \\mathbb EX = \\mathbb EX^2 - \\frac{1}{p} = \\frac{2(1-p)}{p^2} . \\]\nSo\n\\begin{align*}\n\\Var(X) = \\mathbb EX^2 - \\mu^2\n&= \\left(\\frac{2(1-p)}{p^2} + \\frac{1}{p}\\right) - \\left(\\frac{1}{p}\\right)^2 \\\\\n&= \\frac{2(1-p) + p - 1}{p^2} \\\\\n&= \\frac{1-p}{p^2} .\n\\end{align*}\n:::\n\n*Note:* Here, we defined a geometric random variable as being the number of trials up to *and including* the first success, which is a number in $\\{1, 2, \\dots\\}$. However, some authors define it as the number of failures *before* the first success, which is a number in $\\{0, 1, 2,\\dots\\}$. If $X$ is our definition and $Y$ is the second \"number of failures\" definition, then $X$ and $Y+1$ have the same distribution. Annoyingly, R uses the \"number of failures before success\" definition, as we will discuss in a later R worksheet.\n\n\n## Distributions as models for data  {#models}\n\nFamilies of distributions -- like the Bernoulli, binomial and geometric distributions we have seen so far in this module -- are very useful for models in statistics. This idea is developed Bayesian statistics we will discuss in Lectures 19 and 20 of this module, and is an idea that is extremely important throughout the whole MATH1712 Probability and Statistics II.\n\nThe families of distributions we have looked at here are sometimes called \"parametric families\", in that each of the distributions depended on one or more parameters: $p$ for the Bernoulli and geometric distributions; and both $n$ and $p$ for the binomial distribution. (In the next lecture we will see another discrete distribution, the Poisson distribution, and later in the module we will also see some continuous parametric families: the exponential, normal and beta distributions.) This means we can adopt a model that data comes from one of the distributions within a family, then use data to estimate the value of that parameter.\n\nFor example:\n\n* When testing the bias of a coin, you might assume, counting Heads as 1 and Tails as 0, that the outcome of each test is Bernoulli distributed with parameter $p$, but where the value of the Heads probability $p$ is unknown. You could then toss the coin many times and use this data to estimate $p$.\n* The number of years between severe summer floods in a tropical climate could be modelled as geometrically distributed where the flood risk parameter $p$ is unknown. By look at the gaps between severe floods in historical data, a statistician could try to estimate $p$.\n* A tutor might assume that the number of students that turn up to each tutorial is binomially distributed where $n$ is known to be 12, the number of students assigned to the group, but $p$, the \"turning-up probability\" is unknown. The tutor could then take records of how many students turned up to all the tutorials, and use this to estimate $p$.\n\nThere are two main methods statisticians use to estimate parameters:\n\n* **Bayesian statistics:** Here, one starts with a subjective \"prior\" distribution for the parameters, which represents one's personal belief about which possible values for that parameter are more or less likely before conducting any experiment. After the experiment, one then uses Bayes' theorem to update that belief to a \"posterior\" distribution of one's beliefs about the parameter *given* the data. The Bayesian approach will be introduced in Lecture 19 of this module.\n* **Frequentist statistics:** Frequentist statistics does not involve any subjective prior views. Rather, frequentism is about assessing the extent to which the data is consistent with certain hypotheses. For example, one might try to find the value for the parameter that would seem \"most consistent\" with the data, and use that as an \"estimate\" of the parameter: \"My best guess of the Heads probability $p$ of the coin is $p = 0.53$.\" Alternatively, one might find a range of values for the parameter that are all at least somewhat consistent with the data: \"I am confident the value of the Heads probability lies in the range $0.49 \\leq p \\leq 0.57$, as these values are all consistent with the data.\" Third, one could text if a specific hypothesis is consistent with the data or not: \"The data is consistent with the hypothesis that $p = 0.50$, whih would mean the coin is fair, so I have no strong evidence for disbelieving that.\" The frequentist approach will pursued in detail in MATH1712.\n\n\n\n## Summary  {#summary-L11 .unnumbered}\n\n| Distribution | Range | PMF | Expectation | Variance |\n|:----|:-:|:-:|:-:|:-:|\n| **Bernoulli:** $\\text{Bern}(p)$ | $\\{0,1\\}$ | $p(0) = 1- p$, $p(1) = p$ | $p$ | $p(1-p)$ |\n| **Binomial:** $\\text{Bin}(n,p)$ | $\\{0,1,\\dots,n\\}$ | $\\displaystyle\\binom{n}{x} p^x (1-p)^{n-x}$ | $np$ | $np(1-p)$ |\n| **Geometric:** $\\text{Geom}(p)$ | $\\{1,2,\\dots\\}$ | $(1-p)^{x-1}p$ | $\\displaystyle\\frac{1}{p}$ | $\\displaystyle\\frac{1-p}{p^2}$ |\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L11-binomial-geometric.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L11-binomial-geometric.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}