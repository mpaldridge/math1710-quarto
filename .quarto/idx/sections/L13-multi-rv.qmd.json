{"title":"Multiple random variables","markdown":{"headingText":"Multiple random variables ","headingAttr":{"id":"L13-multi-rv","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Joint distributions  {#joint}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/5trzf2sZnLw\"></iframe>\n:::\n::::\n-->\n\n\nIn previous sections, we have looked at single discrete random variables in isolation. In the lecture and the next, we want to look at how multiple discrete random variables can interact.\n\nConsider tossing a fair coin 3 times. Let $X$ be the number of Heads in the first two tosses, and let $Y$ be the number of Heads over all three tosses.\n\nWe know that $X \\sim \\text{Bin}(2, \\frac12)$ and $Y \\sim \\text{Bin}(3, \\frac12)$, so we can easily write down their probability mass functions:\n\n| $x$ | $x = 0$ | $x = 1$ | $x = 2$ |\n|:-:|:-:|:-:|:-:|\n| $p_X(x)$ | $\\frac14$ | $\\frac12$ | $\\frac14$ | \n\n| $y$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ |\n|:-:|:-:|:-:|:-:|:-:|\n| $p_Y(y)$ | $\\frac18$ | $\\frac38$ | $\\frac38$ | $\\frac18$ |\n\nWhen we have multiple random variables and we want to emphasise that a PMF refers to only one of them, we often use the phrase **marginal PMF** or **marginal distribution**. So the PMFs above are the *marginal distributions* of $X$ and $Y$.\n\nHowever, we might also want to know how $X$ and $Y$ interact. To do this, we will need the **joint PMF**, given by\n\\[ p_{X,Y}(x,y) = \\mathbb P(X = x \\text{ and } Y = y) . \\]\n\nIn our case of the coin tosses, we have\n\n| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $\\phantom{p_X(x)}$ |\n|:-:|:-:|:-:|:-:|:-:|:-:|\n| $x=0$ | $\\frac18$ | $\\frac18$ | $0$ | $0$ | |\n| $x=1$ | $0$ | $\\frac14$ | $\\frac14$ | $0$ | |\n| $x=2$ | $0$ | $0$ | $\\frac18$ | $\\frac18$ | |\n| $\\vphantom{p_Y(y)}$ | | | | |\n\nFor just one worked example, we have $p_{X,Y}(2,2) = \\frac18$, because the only way to have $X =2$ (two Heads in the first two coin tosses) and $Y = 2$ (two Heads in the first *three* coin tosses) is to toss Head, Head, Tail, with probability $(\\frac12)^3 = \\frac18$.\n\nFor probabilities of events, we had that if some $A_i$s form a partition, then\n\\[ \\mathbb P(B) = \\sum_i \\mathbb P(B \\cap A_i) . \\]\nNote that the events $\\{X = x\\}$, as $x$ varies over the range of $X$, also make up a partition. Therefore, we have\n\\[ \\mathbb P(Y = y) = \\sum_x \\mathbb P(X = x \\text{ and } Y = y) ; \\]\nor, to phrase this in terms of joint and marginal PMFs,\n\\[ p_Y(y) = \\sum_x p_{X,Y}(x, y) . \\]\nIn other words, to get the marginal distribution of $Y$, we need to sum down the columns in the table of the joint distribution.\n\n| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $\\phantom{p_X(x)}$ |\n|:-:|:-:|:-:|:-:|:-:|:-:|\n| $x=0$ | $\\frac18$ | $\\frac18$ | $0$ | $0$ | |\n| $x=1$ | $0$ | $\\frac14$ | $\\frac14$ | $0$ | |\n| $x=2$ | $0$ | $0$ | $\\frac18$ | $\\frac18$ | |\n| $p_Y(y)$ | $\\frac18$ | $\\frac38$ | $\\frac38$ | $\\frac18$ | |\n\nIn exactly the same way, we have\n\\[ p_X(x) = \\sum_y p_{X,Y}(x, y) ; \\]\n so to get the marginal distribution of $X$, we need to sum across the rows in the table of the joint distribution.\n \n| $p_{X,Y}(x,y)$ | $y = 0$ | $y = 1$ | $y = 2$ | $y = 3$ | $p_X(x)$ |\n|:-:|:-:|:-:|:-:|:-:|:-:|\n| $x=0$ | $\\frac18$ | $\\frac18$ | $0$ | $0$ | $\\frac14$ |\n| $x=1$ | $0$ | $\\frac14$ | $\\frac14$ | $0$ | $\\frac12$ |\n| $x=2$ | $0$ | $0$ | $\\frac18$ | $\\frac18$ | $\\frac14$ |\n| $p_Y(y)$ | $\\frac18$ | $\\frac38$ | $\\frac38$ | $\\frac18$ | |\n\nWe can check that these marginal PMFs match those we started with. (The term \"marginal\" PMF or distribution is presumably because one ends up writing the values in the \"margins\" of the table.)\n\n::: {.thpart}\nIn summary, we have learned the following:\n\n* A joint PMF of two discrete random variables $X$ and $Y$ is\n\\[ p_{X,Y}(x, y) = \\mathbb P(X = x \\text{ and } Y = y) . \\]\n* We can get the marginal distributions $p_X(x) = \\mathbb P(X = x)$ and $p_Y(y) = \\mathbb P(Y = y)$ by summing over the other variable:\n\\[ p_X(x) = \\sum_y p_{X,Y}(x,y) \\qquad p_Y(y) = \\sum_x p_{X,Y}(x,y) . \\]\n:::\n\nNote that the joint PMF conforms to the same rules as a normal PMF:\n\n* it is non-negative: $p_{X,Y}(x,y) \\geq 0$;\n* it sums to 1: $\\displaystyle\\sum_{x,y} p_{X,Y}(x,y) = 1$. \n\nWe may want to look at more than two random variables, $\\mathbf X = (X_1, X_2, \\dots, X_n)$. In this case, the joint PMF is\n\\[ p_{\\mathbf X}(\\mathbf x) = p_{X_1, \\dots, X_n}(x_1, \\dots, x_n) = \\mathbb P(X_1 = x_1 \\text{ and } \\cdots \\text{ and } X_n = x_n) .   \\]\nIn the same way, we can find the marginal distribution of one of the variables -- say $X_1$ -- by summing over all the other variables:\n\\[ p_{X_1}(x) = \\sum_{x_2, \\dots, x_n} p_{X_1, X_2, \\dots, X_n}(x, x_2, \\dots, x_n) . \\]\n\n## Independence of random variables {#independence-rv}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/ZxoVHpnI298\"></iframe>\n:::\n::::\n-->\n\n\nWe said that two *events* are independent if $\\mathbb P(A \\cap B) = \\mathbb P(A)\\, \\mathbb  P(B)$. We now can give a similar definition for what it means two *random variables* to be independent.\n\n::: {.definition}\nWe say two discrete random variables are **independent** if, for all $x$ and $y$, the events $\\{X = x\\}$ and $\\{Y = y\\}$ are independent; that is, if\n\\[ \\mathbb P(X = x \\text{ and } Y = y) = \\mathbb P(X = x) \\, \\mathbb P(Y = y) . \\]\nIn terms of the joint and marginal PMFs, this is the condition that\n\\[ p_{X,Y}(x,y) = p_X(x) \\, p_Y(y) . \\]\n\nMore generally, a sequence of random variables $\\mathbf X = (X_1, X_2, \\dots)$ are independent if\n\\[ p_{\\mathbf X}(\\mathbf x) = p_{X_1}(x_1) \\times p_{X_2}(x_2) \\times \\cdots = \\prod_{i} p_X(x_i). \\]\n:::\n\nReturning to our case of the dice from before, we see that $X$ and $Y$ are *not* independent, because, for just one counterexample, $p_{X,Y}(0,0) = \\frac18$, while $p_X(0) = \\frac14$ and $p_Y(0) = \\frac18$, so $p_{X,Y}(0,0) \\neq p_X(0) \\, p_Y(0)$.\n\nAn important scenario in probability theory and statistics is that of **independent and identically distributed** (or **IID**) random variables. IID random variables represent the same experiment being repeated many times, with each experiment independent of the others. So all the random variables have the same distribution and they are all independent of each other. So $\\mathbf X = (X_1, X_2, \\dots )$ are IID random variables with a common PMF $p_X$, say, if\n\\[ p_{\\mathbf X}(\\mathbf x) = p_X(x_1) \\times p_X(x_2) \\times \\cdots = \\prod_i p_X(x_i) . \\]\n\n::: {.example}\n*Let $X_1, X_2, \\dots, X_{20}$ be IID random variables following a binomial distribution with rate $n = 10$ and $p = 0.3$. What is the probability that all 20 of the the $X_i$ are nonzero?*\n\nBecause the $X_i$ are identically distributed, the probability that any one of them is nonzero is\n\\[ \\mathbb P(X_1 > 0) = 1 - \\mathbb P(X_1 = 0) = 1 - (1 - 0.3)^{10} = 0.972 . \\]\nThen, because the $X_i$ independent, the probability that they are all nonzero is\n\\[ \\mathbb P(X_1 > 0)^{20} = 0.972^{20} = 0.564. \\]\n:::\n\n\n## Conditional distributions  {#cond-rv}\n\n<!--\n:::: {.videowrap}\n::: {.videowrapper}\n<iframe src=\"https://www.youtube.com/embed/U0kNB45K81w\"></iframe>\n:::\n::::\n-->\n\n\nFor probabilities of events, we had the conditional probability\n\\[ \\mathbb P(B \\mid A) = \\frac{\\mathbb P(A \\cap B)}{\\mathbb P(A)} . \\]\nIn the same way, for random variables, we have\n\\[ \\mathbb P(Y = y \\mid X = x) = \\frac{\\mathbb P(X = x \\text{ and } Y = y)}{\\mathbb P(X = x)} . \\]\nWe call the distribution of this the **conditional distribution**.\n\n::: {.definition}\nLet $X$ and $Y$ be two random variables with joint PMF $p_{X,Y}$ and marginal PMFs $p_X$ and $p_Y$ respectively. Then the **condition probability mass function** of $Y$ **given** $X$, $p_{Y \\mid X}$, is given by\n\\[ p_{Y \\mid X}(y \\mid x) = \\frac{p_{X,Y}(x,y)}{p_X(x)} .   \\]\n:::\n\nLet's think again about our coin tossing example. To get the conditional distribution of $Y$ given $X = 1$, say, we have\n\\[ p_{Y \\mid X}(y \\mid 1) = \\frac{p_{X,Y}(1,y)}{p_X(1)} ;   \\]\nso we take the $x = 1$ row of the joint distribution table and \"renormalise it\" by dividing through by the total $p_X(1)$ of the row, so it adds up to 1. That is,\n\\begin{align*}\n  p_{Y \\mid X} (0 \\mid 1) &= \\frac{p_{X,Y}(1, 0)}{p_X(1)} = \\frac{0}{\\frac12} = 0 , \\\\\n  p_{Y \\mid X} (1 \\mid 1) &= \\frac{p_{X,Y}(1, 1)}{p_X(1)} = \\frac{\\frac14}{\\frac12} = \\tfrac12 , \\\\\n  p_{Y \\mid X} (2 \\mid 1) &= \\frac{p_{X,Y}(1, 2)}{p_X(1)} = \\frac{\\frac14}{\\frac12} = \\tfrac12 , \\\\\n  p_{Y \\mid X} (3 \\mid 1) &= \\frac{p_{X,Y}(1, 3)}{p_X(1)} = \\frac{0}{\\frac12} = 0 .\n\\end{align*}\n\nIn just the same way, we could get the conditional distribution of $X$ given $Y = 2$, say, by taking the $y = 2$ column of the joint distribution table, and renormalising by $p_Y(2)$ so that the column sums to 1, That is,\n\\begin{align*}\n  p_{X \\mid Y} (0 \\mid 2) &= \\frac{p_{X,Y}(0,2)}{p_Y(2)} = \\frac{0}{\\frac38} = 0 , \\\\\n  p_{X \\mid Y} (1 \\mid 2) &= \\frac{p_{X,Y}(1,2)}{p_Y(2)} = \\frac{\\frac14}{\\frac38} = \\tfrac23 , \\\\\n  p_{X \\mid Y} (2 \\mid 2) &= \\frac{p_{X,Y}(2,2)}{p_Y(2)} = \\frac{\\frac18}{\\frac38} = \\tfrac13 .\n\\end{align*}\n\nResults that we used for conditional probability with events also carry over to random variables. For example, from Bayes' theorem we know that\n\\[ \\mathbb P(A \\mid B) = \\frac{ \\mathbb P(A) \\, \\mathbb P(B \\mid A)}{\\mathbb P(B)} . \\]\nIn the same way, we have **Bayes' theorem** for random variables:\n\\[ \\mathbb P(X = x \\mid Y = y) = \\frac{ \\mathbb P(X = x) \\, \\mathbb P(Y = y \\mid X = x)}{\\mathbb P(Y = y)} , \\]\nwhich in terms of conditional and marginal PMFs is\n\\[ p_{X \\mid Y}(x \\mid y) = \\frac{p_X(x) \\, p_{Y \\mid X}(y \\mid x)}{p_Y(y)} . \\]\nThis will be a particularly important formula when we study Bayesian statistics at the end of the module.\n\nWe can check Bayes' theorem with $x = 1$ and $y = 2$, for example. \nThe right-hand side of Bayes' theorem is\n\\[ \\frac{p_X(1) \\, p_{Y \\mid X}(2 \\mid 1)}{p_Y(2)} = \\frac{\\frac12 \\times \\frac12}{\\frac38} = \\tfrac{2}{3} .   \\]\nThe left-hand side of Bayes' theorem is\n\\[ p_{X \\mid Y}(1 \\mid 2) = \\tfrac23 , \\]\nwhich is equal, as it should be, to the right-hand side.\n\n\n## Summary  {#summary-L13 .unnumbered}\n\n::: {.mysummary}\n* For two random variables, the joint PMF $p_{X,Y}$, marginal PMF $p_X$, and conditional PMF $p_{Y \\mid X}$ are\n\\begin{align*}\n  p_{X,Y}(x,y) &= \\mathbb P(X =x \\text{ and } Y = y) \\\\\n  p_X(x) &= \\mathbb P(X = x) = \\sum_y p_{X,Y}(x,y) \\\\\n  p_{Y \\mid X}(y \\mid x) &= \\mathbb P(Y = y \\mid X = x) = \\frac{p_{X,Y}(x,y)}{p_X(x)} \n\\end{align*}\n* Two random variables are independent if $p_{X,Y}(x,y) = p_X(x) \\, p_Y(y)$.\n* Bayes' theorem:\n\\[ p_{X \\mid Y}(x \\mid y) = \\frac{ p_X(x) \\, p_{Y \\mid X}(y \\mid x)}{p_Y(y)} . \\]\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"L13-multi-rv.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.475","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}},"pdf":{"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","output-file":"L13-multi-rv.pdf"},"language":{},"metadata":{"block-headings":true,"documentclass":"report"},"extensions":{"book":{}}}}}